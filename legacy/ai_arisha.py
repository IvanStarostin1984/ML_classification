# -*- coding: utf-8 -*-
"""AI_Arisha.ipynb - LEGACY script

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wG0BE_898_erdoSqkj32cNsh6yIKhI00
This file remains for reference and has not been refactored.
"""

# Google Colab single-cell assignment code

# # Dataset upload

# Upload your Kaggle API key to Colab

# from google.colab import files
# files.upload()

# Move the key to the correct directory and set permissions

# !mkdir -p ~/.kaggle
# !cp kaggle.json ~/.kaggle/
# !chmod 600 ~/.kaggle/kaggle.json

# Install Kaggle
# !pip install -q kaggle

# Download the dataset
# !kaggle datasets download -d architsharma01/loan-approval-prediction-dataset

# Unzip the dataset
# !unzip loan-approval-prediction-dataset.zip



# # Dataset Overview

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df_loan = pd.read_csv("loan_approval_dataset.csv")  # Replace with actual CSV file name
df_loan

df = df_loan.copy()

# Basic Info
df.info()

# Check for missing values
df.isna().sum()

# Check for duplicates
df.duplicated().sum()

# Summary Statistics (Numerical)
df.describe()

df.columns

# Strip spaces from column names
df.columns = df.columns.str.strip()

df.columns

# Value Counts for Categorical Columns
print(df['education'].value_counts())
print()
print(df['self_employed'].value_counts())
print()
print(df['loan_status'].value_counts())

df['loan_status'][0]

df['self_employed'][0]

df['education'][0]

# Strip spaces from entries
df['education'] = df['education'].str.strip()
df['self_employed'] = df['self_employed'].str.strip()
df['loan_status'] = df['loan_status'].str.strip()

# # Target variable: Loan status

df['loan_status'] = df['loan_status'].map({'Approved': 1, 'Rejected': 0})

df

# Target variable Distribution
sns.countplot(x='loan_status', data=df)
plt.title('Loan Status Distribution (1 = Approved, 0 = Rejected)')
plt.ylabel('Number of Applicants')
plt.xlabel('Loan Status')
plt.show()

# Class Percentage Distribution
df['loan_status'].value_counts(normalize=True) * 100

df_loan = df.copy()

# # Feature Engineering
# =============================================================================
# FEATURE ENGINEERING  –  tailored for `loan_approval_dataset.csv`
# (Archit Sharma Kaggle dataset, verified April-2025)
# -----------------------------------------------------------------------------
#  Main schema points from the CSV
#  ───────────────────────────────
#   • income_annum                 annual income, single column (₹ / year)
#   • loan_amount                  requested principal (₹)
#   • loan_term                    term in months
#   • no_of_dependents             integer or “3+”
#   • cibil_score                  300 – 900
#   • Assets: residential_assets_value, commercial_assets_value,
#             luxury_assets_value, bank_asset_value
#   • Categoricals: gender, married, education, self_employed, property_area
#   • ✔ NO applicant-/co-applicant split; ✔ NO interest-rate column
# -----------------------------------------------------------------------------
#  Outputs:
#   • df  –  engineered DataFrame ready for train-test split
# =============================================================================

import numpy as np
import pandas as pd
import re, warnings

# ------------------------------------------------------------------
# 0  CONSTANTS
# ------------------------------------------------------------------
MARKET_APR = 0.090          # 9.0 % p.a. (median retail-mortgage rate, Apr-2025)

# ------------------------------------------------------------------
# 1  DEFENSIVE COPY + COLUMN NORMALISATION
# ------------------------------------------------------------------
df_fe = df.copy(deep=True)

df_fe.columns = (
    df_fe.columns
        .str.strip()
        .str.lower()
        .str.replace(r"[ \t\-/]+", "", regex=True)   # drop spaces, dashes
        .str.replace(r"[^\w]", "", regex=True)       # keep a-z 0-9 _
)

def _zeros() -> pd.Series:        # vector of zeros with correct index
    return pd.Series(0.0, index=df_fe.index)

# ------------------------------------------------------------------
# 2  CORE AGGREGATES
# ------------------------------------------------------------------
# 2.1  Total monthly income (₹/month)  – detect any spelling variant
for col in ("income_annum", "incomeannum"):
    if col in df_fe.columns:
        df_fe["total_income_month"] = df_fe[col].fillna(0) / 12.0
        break
else:
    warnings.warn("No `income_annum` column – filling income with zeros.")
    df_fe["total_income_month"] = _zeros()

# 2.2  Total assets
asset_cols = [
    "residential_assets_value",
    "commercial_assets_value",
    "luxury_assets_value",
    "bank_asset_value",
]
for c in asset_cols:
    if c not in df_fe.columns:
        warnings.warn(f"Asset column `{c}` missing – created 0-filled.")
        df_fe[c] = 0.0
df_fe["total_assets"] = df_fe[asset_cols].sum(axis=1)

# 2.3  Net worth
df_fe["net_worth"] = df_fe["total_assets"] - df_fe["loan_amount"]

# ------------------------------------------------------------------
# 3  EQUIVALENT MONTHLY INSTALMENT (EMI) & RATIOS
# ------------------------------------------------------------------
loan_med  = df_fe["loan_amount"].median()
term_med  = df_fe["loan_term"].replace(0, np.nan).median()
if not np.isfinite(term_med) or term_med == 0:
    warnings.warn("loan_term median 0/NaN – defaulting to 120 months.")
    term_med = 120

# 3.1  Simple EMI  (P ÷ n)
df_fe["emi_simple"] = (
    df_fe["loan_amount"].fillna(loan_med) /
    df_fe["loan_term"].replace(0, np.nan).fillna(term_med)
)

# 3.2  Amortised EMI (assume MARKET_APR)
r = MARKET_APR / 12.0
n = df_fe["loan_term"].replace(0, np.nan).fillna(term_med)
P = df_fe["loan_amount"].fillna(loan_med)
df_fe["emi_amortised"] = (
    P * r * (1 + r) ** n / ((1 + r) ** n - 1 + 1e-6)
)

# 3.3  Debt-to-Income (annual) & DSCR
df_fe["debt_to_income_ratio"] = (
    df_fe["loan_amount"] / (df_fe["total_income_month"] * 12 + 1e-6)
)
df_fe["dscr"] = df_fe["total_income_month"] / (df_fe["emi_amortised"] + 1e-6)

# ------------------------------------------------------------------
# 4  LOG-SCALED MONETARY FEATURES
# ------------------------------------------------------------------
df_fe["log_loan_amount"]        = np.log1p(df_fe["loan_amount"])
df_fe["log_total_income_month"] = np.log1p(df_fe["total_income_month"])
df_fe["log_total_assets"]       = np.log1p(df_fe["total_assets"])

# ------------------------------------------------------------------
# 5  CREDIT-SCORE FEATURES
# ------------------------------------------------------------------
df_fe["cibil_score_sq"] = df_fe["cibil_score"] ** 2
cibil_cat = pd.CategoricalDtype(
    ["poor", "fair", "good", "verygood", "excellent"], ordered=True
)
df_fe["cibil_score_bin"] = pd.cut(
    df_fe["cibil_score"],
    bins=[-np.inf, 579, 679, 779, 850, np.inf],
    labels=cibil_cat.categories
).astype(cibil_cat)

# ------------------------------------------------------------------
# 6  LOAN-TERM BUCKETS
# ------------------------------------------------------------------
df_fe["loan_term_bin"] = pd.cut(
    df_fe["loan_term"],
    bins=[0, 9, 12, 18, 24, np.inf],
    labels=["≤9 m", "10–12 m", "13–18 m", "19–24 m", ">24 m"]
)

# ------------------------------------------------------------------
# 7  DEPENDENT & FAMILY FEATURES
# ------------------------------------------------------------------
df_fe["number_of_dependents"] = (
    pd.to_numeric(df_fe.get("no_of_dependents", _zeros()), errors="coerce")
      .fillna(0)
      .astype(int)
)
df_fe["many_dependents_flag"] = (df_fe["number_of_dependents"] >= 3).astype(int)
df_fe["income_per_dependent"] = (
    df_fe["total_income_month"] /
    (df_fe["number_of_dependents"] + 1)
)

# ------------------------------------------------------------------
# 8  ASSET COMPOSITION RATIOS
# ------------------------------------------------------------------
df_fe["luxury_asset_ratio"] = (
    df_fe["luxury_assets_value"] / (df_fe["total_assets"] + 1e-6)
)
liquid_assets = (
    df_fe["bank_asset_value"].fillna(0) +
    df_fe["residential_assets_value"].fillna(0)
)
df_fe["liquid_asset_ratio"] = liquid_assets / (df_fe["total_assets"] + 1e-6)
df_fe["asset_diversity_count"] = (df_fe[asset_cols] > 0).astype(int).sum(axis=1)

# ------------------------------------------------------------------
# 9  EDUCATION & OCCUPATION INTERACTIONS
# ------------------------------------------------------------------
df_fe["graduate_flag"] = (
    df_fe["education"].str.lower().str.contains("graduate")
).astype(int)
df_fe["income_times_graduate"] = df_fe["total_income_month"] * df_fe["graduate_flag"]

df_fe["self_employed_flag"] = (
    df_fe["self_employed"].astype(str).str.lower().isin(["yes", "y", "1"])
).astype(int)
df_fe["selfemp_loan_to_income"] = (
    df_fe["self_employed_flag"] *
    (df_fe["loan_amount"] / (df_fe["total_income_month"] * 12 + 1e-6))
)

# ------------------------------------------------------------------
# 10  HIGH-RISK COMBO FLAG
# ------------------------------------------------------------------
df_fe["highrisk_combo_flag"] = (
    (df_fe["cibil_score"] < 600) &
    (df_fe["loan_amount"] / (df_fe["total_assets"] + 1e-6) > 0.80)
).astype(int)

# ------------------------------------------------------------------
# 11  ONE-HOT ENCODE (education dummies **not** added – graduate_flag kept)
# ------------------------------------------------------------------
cat_cols = [c for c in ["gender", "married", "self_employed", "property_area"]
            if c in df_fe.columns]
if cat_cols:
    df_fe = pd.get_dummies(df_fe, columns=cat_cols,
                           prefix_sep="=", drop_first=True)
bool_cols = df_fe.select_dtypes("bool").columns
df_fe[bool_cols] = df_fe[bool_cols].astype("uint8")
# ------------------------------------------------------------------
# 12  FINAL SANITY CHECK
# ------------------------------------------------------------------
n_missing = int(df_fe.isna().sum().sum())
if n_missing:
    warnings.warn(f"Feature matrix has {n_missing} missing values.")

print(f"✅ Engineered feature matrix ready – shape {df_fe.shape}")
display(df_fe.head())

# hand back
df = df_fe
display(df.head())
df.dtypes.tail()          # confirm self_employed=Yes is uint8
df.isna().sum().sum()
x = np.int64(0)
print(x, type(x), x.dtype)



# ================================================================
#  Categorical Features vs Outcome Status
#  ---------------------------------------------------------------
#  ▸ PURPOSE : quantify & visualise how every categorical predictor
#              relates to the binary target, and persist feature
#              lists for the modelling pipeline.
#  ▸ DELIVERS: • DataFrame  `cat_results`
#              • CSV        `categorical_diagnostics.csv`
#              • JSON       `feature_registry.json`
#              • PNG plots  plots/<feature>.png
# ================================================================
print("Categorical Features vs Outcome Status\n")

import warnings, json, pathlib, math
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from scipy.stats import (
    chi2_contingency,
    fisher_exact,
    MonteCarloMethod,
    norm,
)
from statsmodels.stats.multitest import multipletests
from statsmodels.stats.contingency_tables import Table2x2
from pandas.api.types import CategoricalDtype

# ----------------------------- CONFIG ----------------------------
TARGET        = "loan_status"
verbose_plots = True
palette       = sns.color_palette("colorblind", 2)
plot_max_lvls = 25
rare_freq     = 0.02
mc_n          = 5_000
registry_path = "feature_registry.json"
diag_csv_path = "categorical_diagnostics.csv"
plots_dir     = pathlib.Path("plots")
plots_dir.mkdir(exist_ok=True)

try:
    pd.set_option("future.no_silent_downcasting", True)
except (pd.errors.OptionError, AttributeError):
    pass

# --------------------- 0  enforce category order -----------------
ORDER_SPEC = {
    "cibil_score_bin": ["poor", "fair", "good", "verygood", "excellent"],
    "loan_term_bin":   ["≤9 m", "10–12 m", "13–18 m", "19–24 m", ">24 m"],
}
for col, order in ORDER_SPEC.items():
    if col in df.columns:
        df[col] = (
            df[col].astype(str).str.strip().str.lower()
            .replace({"<=9 m": "≤9 m"})
        )
        df[col] = pd.Categorical(
            df[col],
            categories=[o.lower() for o in order],
            ordered=True,
        )

# ---------------------------- helpers ----------------------------
def _fmt_p(p, thr=1e-6):
    if pd.isna(p): return "NA"
    return f"<{thr:.1e}" if p < thr else f"{p:.4f}"

def _annotate(ax):
    if not hasattr(ax, "bar_label"): return
    for cont in ax.containers:
        data = getattr(cont, "datavalues",
                       [patch.get_height() for patch in cont])
        ax.bar_label(cont, labels=[int(v) for v in data], fontsize=7, padding=2)

def _need_exact(exp):
    flat = exp.ravel()
    return (flat < 1).any() or (flat < 5).sum() / flat.size > .20

def _cramers_v(chi2, tbl):
    r, c = tbl.shape
    return math.sqrt(max(chi2, 0) / (tbl.sum() * (min(r, c) - 1)))

def _cochran_armitage(ct):
    if ct.shape[0] != 2: return np.nan, np.nan
    scores = np.arange(1, ct.shape[1] + 1, dtype=float)
    n1j, n_j = ct.iloc[1].to_numpy(float), ct.sum(axis=0).to_numpy(float)
    n1, n = n1j.sum(), n_j.sum()
    if n == 0: return np.nan, np.nan
    t = (scores * n1j).sum()
    mean_t = n1 * (scores * n_j).sum() / n
    var_t = n1 * (n - n1) * ((n_j * (scores - (scores * n_j).sum() / n) ** 2).sum()) / (n * (n - 1))
    if var_t == 0: return np.nan, np.nan
    z = (t - mean_t) / math.sqrt(var_t)
    p = 2 * (1 - norm.cdf(abs(z)))
    return z, p

def _safe_chi2(ct, need_mc, rng):
    """
    Robust χ² for r×c: handles zero expected cells by 0.5 adjustment.
    Returns (chi2, p, dof, note).
    """
    try:
        if need_mc:
            res = chi2_contingency(
                ct, correction=False,
                method=MonteCarloMethod(n_resamples=mc_n, rng=rng)
            )
            return res.statistic, res.pvalue, res.dof, f"chi2 (MC {mc_n:,})"
        chi2, p, dof, exp = chi2_contingency(ct, correction=False, lambda_=None)
        return chi2, p, dof, "chi2"
    except ValueError:
        # apply Haldane–Anscombe 0.5 adjustment
        adj = ct.to_numpy(float) + 0.5
        chi2, p, dof, _ = chi2_contingency(adj, correction=False)
        return chi2, p, dof, "chi2 (+0.5 adj)"

FRIENDLY = {
    "cibil_score_bin": "Credit-score category",
    "loan_term_bin": "Loan-term bucket",
    "education": "Applicant education",
}

# -------------------------- 1  collect ---------------------------
if TARGET not in df.columns:
    raise KeyError(f"Binary target column '{TARGET}' not found.")
if set(df[TARGET].dropna().unique()) != {0, 1}:
    raise ValueError(f"'{TARGET}' must contain only 0 and 1.")

DUMMY_VARS = [c for c in df.select_dtypes("uint8") if "=" in c]
CAT_MAIN = df.select_dtypes(["object", "category", "bool"]).columns.tolist()
for extra in ("loan_term_bin", "cibil_score_bin"):
    if extra in df.columns:
        CAT_MAIN.append(extra)
CAT_MAIN = [c for c in dict.fromkeys(CAT_MAIN) if c not in (TARGET, *DUMMY_VARS)]
NUMERIC_VARS = [c for c in df.columns if c not in CAT_MAIN + [TARGET] + DUMMY_VARS]
rare_thresh = max(5, math.ceil(rare_freq * len(df)))

# -------------------------- 2  analyse ---------------------------
rows, CONSTANT_VARS = [], []
rng = np.random.default_rng(0)

for col in CAT_MAIN:
    s = df[col].astype("object")
    rare_lvls = s.value_counts(dropna=False)[lambda x: x < rare_thresh].index
    s = s.where(~s.isin(rare_lvls), "other")
    if col in ORDER_SPEC:
        cat_order = [o.lower() for o in ORDER_SPEC[col]] + ["other"]
        s = pd.Categorical(s.str.lower(), categories=cat_order, ordered=True)

    ct = pd.crosstab(s, df[TARGET], dropna=False)
    r, c = ct.shape
    n_rej, n_app = ct.get(0, pd.Series()).sum(), ct.get(1, pd.Series()).sum()
    total = n_rej + n_app
    col1  = ct.get(1, pd.Series(0, index=ct.index))
    app_rates = (col1 / ct.sum(axis=1)).fillna(0)

    # trend
    trend_Z = trend_p = np.nan
    if ct.shape[1] == 2 and (
        (isinstance(s.dtype, CategoricalDtype) and s.ordered) or col.endswith("_bin")
    ):
        trend_Z, trend_p = _cochran_armitage(ct.T)

    # association test
    chi2 = dof = cram_v = or_hat = ci_low = ci_high = np.nan
    if r == c == 2:
        p_val = fisher_exact(ct, alternative="two-sided").pvalue
        note = "Fisher 2x2"
        tbl2 = Table2x2(ct.to_numpy())
        or_hat, (ci_low, ci_high) = tbl2.oddsratio, tbl2.oddsratio_confint()
        try:
            chi2_tmp, *_ = chi2_contingency(ct, correction=False)
        except ValueError:
            chi2_tmp = 0
        cram_v = _cramers_v(chi2_tmp, ct.to_numpy())
    else:
        try:
            _, _, _, exp = chi2_contingency(ct, correction=False)
            need_mc = _need_exact(exp)
        except ValueError:
            need_mc = True
        chi2, p_val, dof, note = _safe_chi2(ct, need_mc, rng)
        cram_v = _cramers_v(chi2, ct.to_numpy()) if np.isfinite(chi2) else np.nan

    rows.append(dict(
        feature=col, test=note,
        n_rejected=n_rej, pct_rejected=n_rej/total if total else np.nan,
        n_approved=n_app, pct_approved=n_app/total if total else np.nan,
        min_pct_app=float(app_rates.min()), max_pct_app=float(app_rates.max()),
        chi2=chi2, dof=dof, cramers_V=cram_v,
        odds_ratio=or_hat, ci_low=ci_low, ci_high=ci_high,
        p_value=p_val, q_value=np.nan,
        trend_Z=trend_Z, trend_p=trend_p, trend_q=np.nan
    ))

    if verbose_plots and ct.shape[0] <= plot_max_lvls and total:
        title = FRIENDLY.get(col, col.replace('_', ' ').title())
        print(f"▶ {title}")
        ax = sns.countplot(x=s, hue=df[TARGET],
                           order=ct.index, hue_order=[0, 1], palette=palette)
        ax.set_title(f"{title}  (p = {_fmt_p(p_val)})", fontsize=11)
        ax.set_ylabel("Applicants"); ax.set_xlabel("")
        plt.setp(ax.get_xticklabels(), rotation=25, ha="right")
        ax.legend(["Rejected", "Approved"], title="Status")
        _annotate(ax)
        plt.tight_layout()
        ax.get_figure().savefig(
            plots_dir / f"{col}.png", dpi=300, bbox_inches="tight"
        )
        plt.show(); plt.close()

# -------------------------- 3  summary table --------------------
cat_results = pd.DataFrame(rows)
if not cat_results.empty:
    if cat_results["p_value"].notna().any():
        cat_results.loc[cat_results["p_value"].notna(), "q_value"] = multipletests(
            cat_results["p_value"].dropna(), method="fdr_bh"
        )[1]
    if cat_results["trend_p"].notna().any():
        cat_results.loc[cat_results["trend_p"].notna(), "trend_q"] = multipletests(
            cat_results["trend_p"].dropna(), method="fdr_bh"
        )[1]
    cat_results = cat_results.sort_values(
        ["p_value", "chi2"], na_position="last"
    ).reset_index(drop=True)
    display(cat_results.style.format({
        "pct_rejected": "{:.1%}", "pct_approved": "{:.1%}",
        "min_pct_app": "{:.1%}", "max_pct_app": "{:.1%}",
        "chi2": "{:.1f}", "cramers_V": "{:.3f}",
        "odds_ratio": "{:.2f}", "ci_low": "{:.2f}", "ci_high": "{:.2f}",
        "trend_Z": "{:.2f}", "trend_p": _fmt_p, "trend_q": _fmt_p,
        "p_value": _fmt_p, "q_value": _fmt_p
    }))
else:
    print("⚠️ No categorical predictors available for analysis.")

cat_results.to_csv(diag_csv_path, index=False)

# -------------------------- 4  registry -------------------------
SIGNIFICANT_VARS = cat_results.loc[
    (
        (cat_results["q_value"].notna()) & (cat_results["q_value"] < 0.05)
    ) | (
        (cat_results["trend_q"].notna()) & (cat_results["trend_q"] < 0.05)
    ),
    "feature"
].tolist()

feature_registry = {
    "numeric": NUMERIC_VARS,
    "categorical": CAT_MAIN,
    "dummy_0_1": DUMMY_VARS,
    "constants": CONSTANT_VARS,
    "significant_categorical": SIGNIFICANT_VARS
}
pathlib.Path(registry_path).write_text(json.dumps(feature_registry, indent=2))

print(f"\nSaved '{registry_path}', '{diag_csv_path}' and "
      f"{len(list(plots_dir.iterdir()))} plot(s) in '{plots_dir}'.")
for k, v in feature_registry.items():
    print(f"  • {k:22s}: {len(v)} column(s)")















# ## Numerical Features Linear Correlation  (tight upgrade v4 – with Top-10 plot)
# ==================================================================
# ▸ Absolute-Spearman clustered heat-map (cluster-aware annotation)
#   ▸ drops ID & ultra-rare binary flags
#   ▸ collapses deterministic twin pairs (|ρ| ≥ 0.98)
#   ▸ annotates strong + FDR-significant cells (|ρ| ≥ 0.75 & q < 0.05)
#   ▸ saves Top-10 unique correlations bar-plot
#   ▸ updates feature_registry.json and writes CSV
# ==================================================================

# ---------- 0  Imports & setup ------------------------------------
import itertools, warnings, json, numpy as np, pandas as pd, seaborn as sns
import matplotlib.pyplot as plt
from pathlib import Path
from scipy import stats
from statsmodels.stats.multitest import multipletests

TARGET = "loan_status"                      # binary 0 / 1
Path("plots").mkdir(exist_ok=True)

# ---------- 1  Numeric predictor list -----------------------------
NUMERIC_VARS = [c for c in df.select_dtypes("number").columns
                if "=" not in c and c != TARGET]

# 1.1  Drop IDs & <1 % binary flags
id_like  = ["loan_id"]
rare_bin = [c for c in NUMERIC_VARS
            if set(df[c].dropna().unique()) <= {0, 1}
            and df[c].astype(float).mean() < .01]
DROP_COLS = id_like + rare_bin

# 1.2  Drop constants / empties
DROP_COLS += [c for c in NUMERIC_VARS
              if df[c].nunique(dropna=True) < 2 or df[c].var() < 1e-12]
if DROP_COLS:
    print("⚠️ dropping:", DROP_COLS)

num_df = df.drop(columns=DROP_COLS)
NUMERIC_VARS = [c for c in NUMERIC_VARS if c not in DROP_COLS]
if len(NUMERIC_VARS) < 2:
    raise RuntimeError("Too few numeric predictors – abort.")

# ---------- 2  NA imputation --------------------------------------
for c in NUMERIC_VARS:
    if num_df[c].isna().any():
        mode = num_df[c].mode(dropna=True)
        fill = num_df[c].mean() if abs(num_df[c].skew()) <= 1 else (
               mode[0] if not mode.empty else 0)
        num_df[c].fillna(fill, inplace=True)

# ---------- 3  Pairwise correlations ------------------------------
pair_records = []
normlike = {c: (abs(num_df[c].skew()) <= 1) for c in NUMERIC_VARS}
for i, j in itertools.combinations(NUMERIC_VARS, 2):
    x, y = num_df[i], num_df[j]
    if x.var() < 1e-12 or y.var() < 1e-12:
        continue
    meth = "pearson" if normlike[i] and normlike[j] else "spearman"
    try:
        coef, p_raw = (stats.pearsonr if meth == "pearson" else stats.spearmanr)(x, y)
    except Exception as e:
        warnings.warn(f"Correlation failed {i}×{j}: {e}")
        continue
    pair_records.append((i, j, coef, p_raw))

corr_df = pd.DataFrame(pair_records, columns=["var1", "var2", "coef", "p_raw"])
corr_df["abs_coeff"] = corr_df["coef"].abs()
corr_df["q"] = multipletests(corr_df["p_raw"], method="fdr_bh")[1]

# ---------- 4  Deterministic twin collapse ------------------------
twin_pairs   = corr_df.query("abs_coeff >= .98")[["var1", "var2"]].values
drop_twins   = {max(p, key=len) for p in twin_pairs}          # keep shorter label
NUMERIC_UNIQUE = [c for c in NUMERIC_VARS if c not in drop_twins]

# ---------- 5  Clustered heat-map ---------------------------------
label_map = {"residential_assets_value": "res_assets",
             "commercial_assets_value":  "comm_assets"}
plot_cols = [label_map.get(c, c) for c in NUMERIC_UNIQUE]
df_plot   = num_df[NUMERIC_UNIQUE].rename(columns=label_map)

mask    = np.triu(np.ones((len(plot_cols), len(plot_cols)), bool))
figsize = (14 if len(plot_cols) > 20 else 10, 10)

cg = sns.clustermap(df_plot.corr("spearman").abs(), cmap="viridis",
                    figsize=figsize, mask=mask, cbar_kws={"label": "|ρ|"})
for tick in cg.ax_heatmap.get_xticklabels():
    tick.set(rotation=45, ha="right")

# 5.1  Look-ups for clustered positions
row_lut = {plot_cols[idx]: pos for pos, idx in enumerate(cg.dendrogram_row.reordered_ind)}
col_lut = {plot_cols[idx]: pos for pos, idx in enumerate(cg.dendrogram_col.reordered_ind)}

# 5.2  Annotate strong + FDR-sig cells
strong = corr_df.query("abs_coeff >= .75 and q < .05")
strong = strong[strong.var1.isin(NUMERIC_UNIQUE) & strong.var2.isin(NUMERIC_UNIQUE)]
for _, r in strong.iterrows():
    a, b = label_map.get(r.var1, r.var1), label_map.get(r.var2, r.var2)
    i, j = row_lut[a], col_lut[b]
    if i < j: i, j = j, i                       # keep in lower triangle
    cg.ax_heatmap.text(j + .5, i + .5, f"{r.abs_coeff:.2f}",
                       ha="center", va="center", fontsize=6, color="black")

cg.fig.suptitle(
    "Absolute Spearman ρ (|ρ|); deterministic twins (|ρ|≥0.98) collapsed.\n"
    "Coefficients ≥0.75 & FDR q<0.05 annotated",
    y=1.03, fontsize=9
)
cg.fig.savefig("plots/corr_heatmap.png", dpi=300, bbox_inches="tight")
plt.close()

# ---------- 6  Top-10 unique correlations bar-plot ---------------
survivors = corr_df[(~corr_df.var1.isin(drop_twins)) & (~corr_df.var2.isin(drop_twins))].copy()
survivors["pair_key"] = survivors.apply(lambda r: " · ".join(sorted([r.var1, r.var2])), axis=1)
survivors = survivors.drop_duplicates("pair_key")
top_df    = survivors.nlargest(10, "abs_coeff").copy()
top_df["pair"] = top_df.apply(lambda r: f"{r.var1} ↔ {r.var2}", axis=1)

if not top_df.empty:
    plt.figure(figsize=(6, 0.4*len(top_df)+1))
    sns.barplot(data=top_df, x="abs_coeff", y="pair", orient="h", color="steelblue")
    plt.xlabel("|ρ|"); plt.ylabel("")
    plt.title("Top-10 absolute Spearman correlations (twins collapsed)")
    plt.tight_layout()
    plt.savefig("plots/corr_top10.png", dpi=300, bbox_inches="tight")
    plt.close()

# ---------- 7  Registry & CSV ------------------------------------
high_pairs = [(r.var1, r.var2) for _, r in corr_df.iterrows() if r.abs_coeff >= .90]
reg_path   = Path("feature_registry.json")
registry   = json.loads(reg_path.read_text()) if reg_path.exists() else {}
def _dedup(old, new): return sorted({tuple(sorted(p)) for p in old}|{tuple(sorted(p)) for p in new})
registry["numeric_high_corr_pairs"] = _dedup(registry.get("numeric_high_corr_pairs", []), high_pairs)
registry["numeric_to_drop_lm"]      = sorted(drop_twins)
reg_path.write_text(json.dumps(registry, indent=2))

corr_df.to_csv("numerical_correlations.csv", index=False)
print(f"✅ Correlation scan complete → {len(NUMERIC_UNIQUE)} vars "
      f"(dropped {len(drop_twins)} twins & {len(DROP_COLS)} trivial cols); "
      "heat-map and Top-10 plots saved in /plots/")






# ## Encoding categorical features
# ## Encoding categorical features
#
# Produces two ready-to-use transformers:
#   • prep_lr   – scaling for numerical inputs (Logistic Regression)
#   • prep_tree – no scaling (tree-based models)
# and the helper `safe_transform()` that prevents schema-mismatch crashes
# when new columns appear at prediction time.

import warnings
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import (
    StandardScaler,
    OrdinalEncoder,
    OneHotEncoder,
)

# ------------------------------------------------------------------
# 1.  Column-type discovery (data-driven, schema-agnostic)
# ------------------------------------------------------------------
TARGET = "loan_status"                       # change only if target differs

cat_cols  = df.select_dtypes(["object", "category"]).columns.tolist()
bool_cols = df.select_dtypes("bool").columns.tolist()

def _is_binary_numeric(series: pd.Series) -> bool:
    """Return True if numeric series contains only 0/1."""
    if series.dtype.kind not in "if":              # not int/float
        return False
    return set(series.dropna().unique()) <= {0, 1}

num_cols = [
    c for c in df.columns
    if c not in cat_cols + bool_cols + [TARGET] and not _is_binary_numeric(df[c])
]

# uint8 dummy columns and uint8 flag columns remain in the dataframe;
# they will pass through unchanged (no duplicate encoding).

# ------------------------------------------------------------------
# 2.  Ordinal mapping and validation
# ------------------------------------------------------------------
ORDINAL_MAP = {
    "cibil_score_bin": ["poor", "fair", "good", "verygood", "excellent"],
    "loan_term_bin"  : ["≤9 m", "10–12 m", "13–18 m", "19–24 m", ">24 m"],
}

for col, order in ORDINAL_MAP.items():
    if col in df.columns:
        df[col]          = df[col].astype(str).str.strip().str.lower()
        ORDINAL_MAP[col] = [v.lower().strip() for v in order]
        missing = set(df[col].dropna().unique()) - set(ORDINAL_MAP[col])
        if missing:
            raise ValueError(f"Ordinal map for '{col}' lacks {sorted(missing)}")

ordinal_cols = [c for c in ORDINAL_MAP if c in df.columns]
nominal_cols = [c for c in cat_cols if c not in ordinal_cols]

# ------------------------------------------------------------------
# 3.  Encoder pipelines (impute → encode)
# ------------------------------------------------------------------
min_freq = min(0.05, max(0.01, 5 / len(df)))       # 1 % ≤ freq ≤ 5 %

if ordinal_cols:
    ord_pipe = Pipeline([
        ("impute", SimpleImputer(strategy="most_frequent")),
        ("encode", OrdinalEncoder(
            categories=[ORDINAL_MAP[c] for c in ordinal_cols],
            handle_unknown="use_encoded_value",
            unknown_value=-1,
            encoded_missing_value=-1,
            dtype="int32",
        )),
    ])
else:
    ord_pipe = "passthrough"

ohe_lr = OneHotEncoder(
    drop="first",
    handle_unknown="infrequent_if_exist",
    min_frequency=min_freq,
    max_categories=50,
    sparse_output=False,
)

ohe_tree = OneHotEncoder(
    drop=None,
    handle_unknown="infrequent_if_exist",
    min_frequency=min_freq,
    max_categories=50,
    sparse_output=False,
)

# ------------------------------------------------------------------
# 4.  ColumnTransformers
# ------------------------------------------------------------------
prep_lr = ColumnTransformer(
    transformers=[
        ("num",   StandardScaler(), num_cols),
        ("bool",  "passthrough",    bool_cols),
        ("ord",   ord_pipe,         ordinal_cols),
        ("nom",   ohe_lr,           nominal_cols),
    ],
    remainder="passthrough",
)

prep_tree = ColumnTransformer(
    transformers=[
        ("num_bool", "passthrough", num_cols + bool_cols),
        ("ord",      ord_pipe,      ordinal_cols),
        ("nom",      ohe_tree,      nominal_cols),
    ],
    remainder="passthrough",
)

# ------------------------------------------------------------------
# 5.  Safe transform helper — drops unseen columns before transform
# ------------------------------------------------------------------
def safe_transform(
    preprocessor: ColumnTransformer,
    X_new: pd.DataFrame,
    copy: bool = True,
):
    """
    Transform X_new with a fitted ColumnTransformer.
    Unknown columns are dropped (with warning) *before* transform(),
    preventing scikit-learn ≥1.2 schema errors.

    Parameters
    ----------
    preprocessor : ColumnTransformer
        A fitted transformer (prep_lr or prep_tree).

    X_new : pandas.DataFrame
        New data to transform.

    copy : bool, default True
        Operate on a copy of X_new[common] to avoid modifying caller data.

    Returns
    -------
    ndarray
        Encoded feature matrix.
    """
    if not isinstance(X_new, pd.DataFrame):
        raise TypeError("safe_transform expects a pandas DataFrame.")

    common = X_new.columns.intersection(preprocessor.feature_names_in_)
    extras = set(X_new.columns) - set(common)
    if extras:
        warnings.warn(
            f"dropped unseen columns at predict-time: {sorted(extras)}",
            RuntimeWarning,
        )

    X_use = X_new[common].copy() if copy else X_new[common]
    return preprocessor.transform(X_use)

# --- smoke test for the encoders ---------------------------------
prep_lr.fit(df.drop(columns="loan_status"), df["loan_status"])
X_enc = prep_lr.transform(df.drop(columns="loan_status"))

print("Encoded shape :", X_enc.shape)          # expect ≈ (4269, ≈40)
print("Ordinal cols  :", len(prep_lr.named_transformers_["ord"]
                             .named_steps["encode"].categories_))
print("Sample row    :", X_enc[0][:12])        # first 12 features just as a peek



# # ML model training

# ================================================================
#  ## Feature selection & check
#  ---------------------------------------------------------------
#  Exact, self-contained code block — copy & run.
#  Version: v11.5  (all runtime fixes) — 30 Apr 2025
# ================================================================

# ── 0  Imports & configuration ──────────────────────────────────
import json, re, time, zlib, hashlib
from pathlib import Path
import numpy as np, pandas as pd
from pandas.api.types import is_numeric_dtype
from statsmodels.stats.outliers_influence import variance_inflation_factor as vif
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import RFECV, SelectFromModel
from sklearn.pipeline import Pipeline
from sklearn.ensemble import ExtraTreesClassifier

SEED                   = 0
THRESH_DUP_MODE        = 0.99          # ≥99 % identical ⇒ quasi-constant
THRESH_CONST_VAR       = 1e-12
THRESH_PAIRWISE_R      = 0.85
THRESH_VIF_PRIMARY     = 10
THRESH_VIF_OPTIONAL    = 5             # when n < 10 × p
RFECV_STEP             = 2
USE_SIG_CAT            = True
ID_REGEX               = r"(?:_?id$|_?no$|loan_id|customerid?|customer_reference)"
np.random.seed(SEED)

# expected globals: df, prep_lr, prep_tree, cat_results
if "cat_results" not in globals():
    cat_results = pd.DataFrame()       # safeguard first run

# ── 1  Registry ─────────────────────────────────────────────────
reg_path = Path("feature_registry.json")
reg = json.loads(reg_path.read_text()) if reg_path.exists() else {}

# ── 2  Stopwatch ────────────────────────────────────────────────
t0 = time.perf_counter()

# ── 3  Raw predictors list ─────────────────────────────────────
if reg:
    predictors = (
          reg.get("numeric", [])
        + reg.get("categorical", [])
        + reg.get("dummy_0_1", [])
    )
    predictors = [c for c in predictors if c not in reg.get("numeric_to_drop_lm", [])]
else:
    predictors = df.columns[df.columns != "loan_status"].tolist()

assert df.index.is_unique, "DataFrame index must be unique"

X_full = df[predictors].copy()
print(f"Candidate matrix: {X_full.shape[0]:,} × {X_full.shape[1]}")

pre_sel_sha = pd.util.hash_pandas_object(df, index=True).values
reg.update({
    "n_rows_raw": len(df),
    "n_cols_raw": df.shape[1],
    "pre_selection_sha256": hashlib.sha256(pre_sel_sha).hexdigest(),
})

# ── 4  Quasi-constants / ID-like ────────────────────────────────
drop_const = []
for c in X_full.columns:
    s = X_full[c]
    low_var = is_numeric_dtype(s) and (s.var() < THRESH_CONST_VAR)
    few_lvl = s.nunique(dropna=True) < 2
    high_mod = s.value_counts(normalize=True, dropna=False).iloc[0] >= THRESH_DUP_MODE
    id_like = re.search(ID_REGEX, c, flags=re.I) is not None
    if low_var or few_lvl or high_mod or id_like:
        drop_const.append(c)

X_full.drop(columns=drop_const, inplace=True)
n_quasi   = len(drop_const)
pct_quasi = 100 * n_quasi / max(1, len(predictors))
quasi_msg = f"{n_quasi} ({pct_quasi:.1f}%)"
print("Dropped quasi-constant / ID cols :", quasi_msg)
reg["n_quasi_constant"] = n_quasi

# ── 5  Multicollinearity ───────────────────────────────────────
num_cols = X_full.select_dtypes("number").columns

# 5.a  pairwise |ρ|
corr_mat = X_full[num_cols].corr("spearman").abs()
high_pairs = (
    corr_mat.where(np.triu(np.ones_like(corr_mat, bool), 1))
            .stack()
            .loc[lambda s: s >= THRESH_PAIRWISE_R]
            .sort_values(ascending=False)
)
to_drop = set()
for (a, b), _ in high_pairs.items():
    if a in to_drop or b in to_drop:
        continue
    to_drop.add(a if corr_mat[a].mean() > corr_mat[b].mean() else b)

X_full.drop(columns=list(to_drop), inplace=True)

num_cols = X_full.select_dtypes("number").columns   # refresh
if num_cols.size == 0:
    max_r = 0.0
else:
    tri = X_full[num_cols].corr("spearman").abs().values
    max_r = tri[np.triu_indices_from(tri, 1)].max() if tri.size > 1 else 0.0
print(f"Max |ρ| after pruning: {max_r:.3f}")

# 5.b  VIF
def _vif_prune(cols, cap):
    while True:
        arr  = X_full[cols].to_numpy(float)
        vifs = pd.Series([vif(arr, i) for i in range(arr.shape[1])], index=cols)
        if vifs.max() <= cap or len(cols) < 2:
            return cols, vifs
        cols.remove(vifs.idxmax())

num_keep, vif_out = _vif_prune(list(num_cols), THRESH_VIF_PRIMARY)
if len(df) < 10 * len(num_keep):
    print(f"VIF ceiling tightened to {THRESH_VIF_OPTIONAL} (n < 10 × p)")
    num_keep, vif_out = _vif_prune(num_keep, THRESH_VIF_OPTIONAL)

max_vif = float(vif_out.max()) if not vif_out.empty else 0.0
print(f"Max VIF after pruning: {max_vif:.2f}")
print("⚠ Dummy vars may still raise VIF in LR; inspect diagnostics")

reg.update({
    "n_high_corr_dropped": len(to_drop),
    "max_residual_rho":    max_r,
    "max_residual_vif":    max_vif,
})

# ── 6  Categorical relevance (χ² / trend) ──────────────────────
sig_cat = []
need = {'q_value', 'trend_q', 'n_approved', 'n_rejected'}
if USE_SIG_CAT and need.issubset(cat_results.columns):
    sizes = cat_results['n_approved'] + cat_results['n_rejected']
    if sizes.min() < 10:
        print("Trend test skipped – level <10 observations")
        cat_results[['trend_p', 'trend_q']] = np.nan
    sig_cat = cat_results.query("(q_value < 0.05) or (trend_q < 0.05)")["feature"].tolist()

reg["sig_cat_used"] = bool(USE_SIG_CAT)
cat_keep_lr = sig_cat if USE_SIG_CAT else []

# ── 7  Post-filter provenance ──────────────────────────────────
post_feat_df = X_full.copy()
reg.update({
    "n_rows_postfeature": len(post_feat_df),
    "n_cols_postfeature": post_feat_df.shape[1],
    "post_feature_sha256": hashlib.sha256(
        pd.util.hash_pandas_object(post_feat_df, index=True).values
    ).hexdigest(),
})

if len(df) <= 1_000_000:
    reg["dataset_content_sha256"] = hashlib.sha256(
        pd.util.hash_pandas_object(df.sort_values(df.columns.tolist()), index=False).values
    ).hexdigest()
else:
    reg["dataset_content_sha256"] = f"CRC32:{zlib.crc32(df.to_numpy().tobytes()):08X}"

# ── 8  Selector pipelines ──────────────────────────────────────
base_lr = LogisticRegression(
    solver="saga", penalty="l1", C=1,
    class_weight="balanced", max_iter=4000,
    n_jobs=-1, random_state=SEED
)
pipe_lr = Pipeline([
    ("prep",  prep_lr),
    ("rfe",   RFECV(base_lr, step=RFECV_STEP, cv=5, scoring="roc_auc")),
    ("clf",   base_lr.set_params(penalty="l2"))
])

selector_trees = ExtraTreesClassifier(
    n_estimators=250, n_jobs=-1,
    class_weight="balanced", random_state=SEED
)
clf_trees = ExtraTreesClassifier(
    n_estimators=250, n_jobs=-1,
    class_weight="balanced", random_state=SEED
)
pipe_tree = Pipeline([
    ("prep",   prep_tree),
    ("select", SelectFromModel(selector_trees, threshold="mean")),
    ("clf",    clf_trees)
])

print(f"Feature-selection prep assembled in {time.perf_counter() - t0:.1f} s")

# ── 9  Hand-off matrices ───────────────────────────────────────
X_lr   = post_feat_df.copy()
X_tree = post_feat_df.copy()
y      = df["loan_status"]

assert X_lr.isna().sum().sum() == 0
assert X_tree.isna().sum().sum() == 0

pct_collin = 100 * len(to_drop) / max(1, len(predictors))

# ── 10  Save registry & report ──────────────────────────────────
reg["selection_timestamp"] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
reg_path.write_text(json.dumps(reg, indent=2))

print(f"⏱ Feature-selection cell done in {time.perf_counter() - t0:.1f} s")
print(f"LogReg features : {X_lr.shape}")
print(f"Tree   features : {X_tree.shape}")
print(f"Dropped quasi-constant / ID cols : {quasi_msg}")
print(f"Dropped high-ρ numeric cols      : {len(to_drop)} ({pct_collin:.1f}%)")
print(f"Max residual |ρ| : {max_r:.3f} │ Max residual VIF : {max_vif:.2f}")





# ================================================================
# ## Data splitting  (+ leak-free re-calculation of significant
#                      categorical predictors on TRAIN only)
# ----------------------------------------------------------------
#  • Creates a single, frozen train–test split
#  • Random-stratified or chronological (DATE_COL)
#  • Guarantees ≥100 minority rows in the training fold
#  • Persists indices & CSV checksum
#  • ‼ Recomputes χ² / trend tests **on the training set only** and
#    over-writes `cat_results`, `SIGNIFICANT_VARS`, and the registry
# ================================================================

# ── 0  CONSTANTS ────────────────────────────────────────────────
SEED               = 0
TEST_SIZE          = 0.15           # initial hold-out fraction
SPLIT_METHOD       = "random"       # "random" | "time"
DATE_COL           = None           # eg. "application_date"
TARGET_COL         = "loan_status"
MIN_TRAIN_MINORITY = 80            # hard floor

# ── 1  IMPORTS ──────────────────────────────────────────────────
import pathlib, hashlib, json, datetime as _dt, warnings, re, math
import numpy as np, pandas as pd
from sklearn.model_selection import train_test_split
from scipy.stats import chi2_contingency, fisher_exact, norm
from statsmodels.stats.multitest import multipletests
from pandas.api.types import CategoricalDtype

# df (feature-engineered, incl. TARGET_COL) must already exist
if TARGET_COL not in df.columns:
    raise KeyError(f"Column '{TARGET_COL}' not found in df.")

# ── 2  CSV CHECKSUM ─────────────────────────────────────────────
RAW_CSV_PATH = next(
    (p for p in pathlib.Path(".").glob("**/*.csv")
     if re.search(r"loan.*dataset", p.name, flags=re.I)), None)
if RAW_CSV_PATH is None:
    raise FileNotFoundError("Original CSV file not located.")
sha256_csv = hashlib.sha256(RAW_CSV_PATH.read_bytes()).hexdigest()

# ── 3  CLASS BALANCE ────────────────────────────────────────────
n_total      = len(df)
class_counts = df[TARGET_COL].value_counts(dropna=False).to_dict()
print(f"Dataset size : {n_total:,} rows")
print("Class counts :", class_counts)

minority_class, minority_n = min(class_counts.items(), key=lambda kv: kv[1])
if minority_n < 100 or minority_n / n_total < 0.05:
    warnings.warn("Minority class <100 rows or <5 %; expect wide CIs.",
                  RuntimeWarning)

# ── 4  SPLITTERS ────────────────────────────────────────────────
def _random_split(test_frac: float):
    _, test_idx = train_test_split(df.index,
                                   test_size=test_frac,
                                   stratify=df[TARGET_COL],
                                   random_state=SEED)
    return df.index.difference(test_idx), test_idx

def _time_split(test_frac: float):
    if DATE_COL is None or DATE_COL not in df.columns:
        raise ValueError("DATE_COL must be set & present for time split.")
    if df[DATE_COL].isna().any():
        raise ValueError(f"{DATE_COL} contains NaNs.")
    df_sorted = df.sort_values(DATE_COL, kind="mergesort")
    if not df_sorted[DATE_COL].is_monotonic_increasing:
        raise ValueError(f"{DATE_COL} is not strictly increasing.")
    cut       = int(round((1 - test_frac) * n_total))
    return df_sorted.index[:cut], df_sorted.index[cut:]

_split = _time_split if SPLIT_METHOD.lower() == "time" else _random_split

# ── 5  CREATE SPLIT (ensure ≥100 minority rows in train) ───────
curr_test = TEST_SIZE
for _ in range(5):
    train_idx, test_idx = _split(curr_test)
    train_min = df.loc[train_idx, TARGET_COL].value_counts().min()
    if train_min >= MIN_TRAIN_MINORITY:
        break
    curr_test = max(0.05, curr_test - 0.05)
else:
    warnings.warn("Training minority class remains below threshold.",
                  RuntimeWarning)

print(f"Split method  : {SPLIT_METHOD}  |  test_size = {curr_test:.2f}")

# ── 6  SAVE ARTEFACTS ───────────────────────────────────────────
ART = pathlib.Path("artefacts"); ART.mkdir(exist_ok=True)
np.savez(ART / "split_idx.npz",
         train=train_idx.to_numpy(), test=test_idx.to_numpy())

with open(ART / "split_info.json", "w") as f:
    json.dump({
        "timestamp_utc": _dt.datetime.utcnow().isoformat(timespec="seconds")+"Z",
        "seed": SEED,
        "method": SPLIT_METHOD,
        "test_size": curr_test,
        "date_col": DATE_COL,
        "n_total": n_total,
        "class_counts": {str(k): int(v) for k, v in class_counts.items()},
        "train_rows": int(train_idx.size),
        "test_rows":  int(test_idx.size),
        "train_minority_rows": int(train_min),
        "sha256_csv": sha256_csv,
    }, f, indent=2)
print("Artefacts saved to:", ART.resolve())

# ── 7  HAND-OFF MATRICES ────────────────────────────────────────
train_df = df.loc[train_idx].reset_index(drop=True)
test_df  = df.loc[test_idx].reset_index(drop=True)

X_tr, y_tr = train_df.drop(columns=TARGET_COL), train_df[TARGET_COL]
X_te, y_te = test_df.drop(columns=TARGET_COL),  test_df[TARGET_COL]

print(f"Training set : {X_tr.shape[0]:,} rows "
      f"({y_tr.value_counts(normalize=True).mul(100).round(1).to_dict()})")
print(f"Test set     : {X_te.shape[0]:,} rows "
      f"({y_te.value_counts(normalize=True).mul(100).round(1).to_dict()})")

# ────────────────────────────────────────────────────────────────
#  >>  RECOMPUTE χ² / TREND TESTS ON TRAINING DATA ONLY  <<
# ----------------------------------------------------------------
print("\nRecomputing significant categorical predictors on TRAIN set…")

# 1  collect categorical columns (definitions from earlier cells)
DUMMY_VARS = [c for c in train_df.select_dtypes("uint8") if "=" in c]
CAT_MAIN   = train_df.select_dtypes(["object", "category", "bool"]).columns.tolist()
for extra in ("loan_term_bin", "cibil_score_bin"):
    if extra in train_df.columns: CAT_MAIN.append(extra)
CAT_MAIN = [c for c in dict.fromkeys(CAT_MAIN)
            if c not in (TARGET_COL, *DUMMY_VARS)]
rare_thresh = max(5, math.ceil(0.01 * len(train_df)))

rows = []
rng  = np.random.default_rng(0)

for col in CAT_MAIN:
    s = train_df[col].astype("object")
    rare_lvls = s.value_counts(dropna=False)[lambda x: x < rare_thresh].index
    s = s.where(~s.isin(rare_lvls), "other")

    ct = pd.crosstab(s, train_df[TARGET_COL], dropna=False)
    r, c = ct.shape
    if r < 2 or c < 2: continue     # skip degenerate tables

    # choose exact or chi²
    if r == c == 2:
        p_val = fisher_exact(ct, alternative="two-sided").pvalue
    else:
        try:
            p_val = chi2_contingency(ct, correction=False)[1]
        except ValueError:
            p_val = np.nan

    rows.append((col, p_val))

cat_results_tr = (
    pd.DataFrame(rows, columns=["feature", "p_value"])
      .assign(q_value=lambda d: multipletests(d["p_value"],
                                              method="fdr_bh")[1])
      .sort_values("q_value")
      .reset_index(drop=True)
)

SIGNIFICANT_VARS = cat_results_tr.query("q_value < 0.05")["feature"].tolist()
cat_results      = cat_results_tr        # overwrite global used later

print(f"Significant (q<0.05) categorical vars in TRAIN: {len(SIGNIFICANT_VARS)}")
if SIGNIFICANT_VARS:
    print(" →", SIGNIFICANT_VARS)

# update on-disk registry so downstream cells see the train-only list
reg_path = pathlib.Path("feature_registry.json")
if reg_path.exists():
    registry = json.loads(reg_path.read_text())
    registry["significant_categorical"] = SIGNIFICANT_VARS
    reg_path.write_text(json.dumps(registry, indent=2))




# ## Class balancing
# ================================================================
#  * uses df, X_tr, y_tr, X_te, y_te, prep_lr, prep_tree, set_seeds
#  * selects best Logistic-Regression & Decision-Tree with resampling
#  * saves every artefact needed by the later notebook sections
# ================================================================

# ---------- 0  CONFIG -------------------------------------------------------
SEED               = 0
TARGET             = "loan_status"            # 1 = Approved, 0 = Rejected
IR_THRESHOLD       = 1.2                      # lower to 1.2 if recall critical
ORDER              = "scaler→sampler"         # encoder first ⇒ samplers see numbers
FORCE_CATEGORICAL  = []                       # e.g. ["zip_code"]
FORCE_NUMERIC      = []                       # e.g. ["rating_1to5"]
assert ORDER in {"sampler→scaler", "scaler→sampler"}

# ---------- 1  IMPORTS + SEEDS ----------------------------------------------
import os, random, json, joblib, numpy as np, pandas as pd
from pathlib import Path
from sklearn.metrics import (roc_auc_score, average_precision_score,
                             f1_score, recall_score, balanced_accuracy_score)
from sklearn.model_selection import (GridSearchCV, RepeatedStratifiedKFold,
                                     ParameterGrid)
from sklearn.linear_model    import LogisticRegression
from sklearn.tree            import DecisionTreeClassifier
from imblearn.pipeline       import Pipeline
from imblearn.over_sampling  import SMOTE, SMOTENC, SMOTEN, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine        import SMOTETomek
from sklearn.metrics import confusion_matrix

def set_seeds(s=SEED):
    random.seed(s); np.random.seed(s); os.environ["PYTHONHASHSEED"] = str(s)
set_seeds()

# ---------- 2  IMBALANCE CHECK ----------------------------------------------
vc = y_tr.value_counts(dropna=False)
if len(vc) != 2:
    raise ValueError("TARGET must be binary.")
maj, min_ = int(vc.max()), int(vc.min())
IR = maj / min_
RESAMPLE = IR >= IR_THRESHOLD
print(f"{'🟢' if RESAMPLE else '⚪'} majority={maj:,}  minority={min_:,}  "
      f"IR={IR:.2f}  {'≥' if RESAMPLE else '<'}{IR_THRESHOLD}  "
      f"→ RESAMPLING {'ON' if RESAMPLE else 'OFF'}")

# ---------- 3  FEATURE-TYPE DISCOVERY ---------------------------------------
num_cols = df.select_dtypes("number").columns.tolist()
cat_cols = df.select_dtypes(["object", "category"]).columns.tolist()
for col in df.select_dtypes("int").columns:
    if col in FORCE_NUMERIC:
        continue
    range_ = df[col].max() - df[col].min()
    if (col in FORCE_CATEGORICAL) or (df[col].nunique() <= 20 and range_ < 100):
        if col not in cat_cols:
            cat_cols.append(col)
        if col in num_cols:
            num_cols.remove(col)
cat_mask   = [i for i, c in enumerate(X_tr.columns) if c in cat_cols]
minority_n = min_

# ---------- 4  SAMPLER GRID --------------------------------------------------
if not RESAMPLE:
    sampler_grid = ["passthrough"]
else:
    if minority_n < 6:
        base_sampler = RandomOverSampler(random_state=SEED)
    elif cat_cols and num_cols:
        base_sampler = SMOTENC(categorical_features=cat_mask, random_state=SEED)
    elif cat_cols:
        base_sampler = SMOTEN(random_state=SEED)
    else:
        base_sampler = SMOTE(random_state=SEED)
    sampler_grid = ["passthrough", base_sampler,
                    RandomUnderSampler(random_state=SEED),
                    SMOTETomek(random_state=SEED)]
assert len(set(sampler_grid)) == len(sampler_grid)

# ---------- 5  PIPELINE BUILDERS --------------------------------------------
def lr_steps(samp):
    return ([("prep", prep_lr), ("sampler", samp), ("clf", None)]
            if ORDER == "scaler→sampler"
            else [("sampler", samp), ("prep", prep_lr), ("clf", None)])

def tree_steps(samp):
    return [("prep", prep_tree), ("sampler", samp), ("clf", None)]

# ---------- 6  PARAMETER GRIDS ----------------------------------------------
samplers_real = [s for s in sampler_grid if s != "passthrough"]

param_grid_lr = [
    {"sampler": ["passthrough"],
     "clf__C": [0.3, 1, 3],
     "clf__penalty": ["l1", "l2"],
     "clf__class_weight": ["balanced"]},
    {"sampler": samplers_real,
     "clf__C": [0.3, 1, 3],
     "clf__penalty": ["l1", "l2"],
     "clf__class_weight": [None]},
]

param_grid_tree = [
    {"sampler": ["passthrough"],
     "clf__max_depth": [None, 8, 15],
     "clf__min_samples_leaf": [1, 5],
     "clf__class_weight": [None, "balanced"]},
    {"sampler": samplers_real,
     "clf__max_depth": [None, 8, 15],
     "clf__min_samples_leaf": [1, 5],
     "clf__class_weight": [None]},
]

TOTAL_FITS = len(ParameterGrid(param_grid_lr)) + len(ParameterGrid(param_grid_tree))
MAX_FITS   = max(16, min(300, int(0.10 * len(X_tr))))
if TOTAL_FITS > MAX_FITS:
    raise ValueError(f"{TOTAL_FITS} fits exceed cap {MAX_FITS}")
print(f"Grid size LR={len(ParameterGrid(param_grid_lr))}  "
      f"Tree={len(ParameterGrid(param_grid_tree))}  (total {TOTAL_FITS})")

# ---------- 7  CROSS-VALIDATION ---------------------------------------------
if minority_n < 2:
    raise ValueError("Only one minority row – cannot stratify.")
cv = RepeatedStratifiedKFold(n_splits=min(5, minority_n),
                             n_repeats=3, random_state=SEED)

# ---------- 8  GRID SEARCH ---------------------------------------------------
def run_gs(name, steps, estimator, grid):
    pipe = Pipeline(steps); pipe.set_params(clf=estimator)
    gs = GridSearchCV(pipe, grid, cv=cv, scoring="roc_auc",
                      n_jobs=-1, verbose=1, refit=True)
    gs.fit(X_tr, y_tr)
    print(f"✔ {name} best ROC-AUC = {gs.best_score_:.4f}")
    return gs

gs_lr   = run_gs("LogReg", lr_steps("passthrough"),
                 LogisticRegression(solver="saga", max_iter=4000, random_state=SEED),
                 param_grid_lr)

gs_tree = run_gs("Tree",   tree_steps("passthrough"),
                 DecisionTreeClassifier(random_state=SEED),
                 param_grid_tree)

# ---------- 9  SAVE ARTEFACTS -----------------------------------------------
Path("artefacts").mkdir(exist_ok=True)
pd.DataFrame(gs_lr.cv_results_ ).to_csv("cv_results_lr_2025-04-30.csv",   index=False)
pd.DataFrame(gs_tree.cv_results_).to_csv("cv_results_tree_2025-04-30.csv",index=False)
joblib.dump(gs_lr.best_estimator_,   "best_lr_2025-04-30.pkl")
joblib.dump(gs_tree.best_estimator_, "best_tree_2025-04-30.pkl")
json.dump([str(s) for s in sampler_grid],
          open("sampler_grid_2025-04-30.json", "w"))

# ---------- 10  TEST-SET METRICS --------------------------------------------
best_lr, best_tree = gs_lr.best_estimator_, gs_tree.best_estimator_
y_lr_prob  = best_lr.predict_proba(X_te)[:, 1]
y_tree_prob = best_tree.predict_proba(X_te)[:, 1]
y_lr_pred  = (y_lr_prob  >= 0.5)
y_tree_pred = (y_tree_prob >= 0.5)

np.save("y_prob_lr.npy",   y_lr_prob)
np.save("y_prob_tree.npy", y_tree_prob)
np.save("y_pred_lr.npy",   y_lr_pred)
np.save("y_pred_tree.npy", y_tree_pred)

def show_metrics(lbl, y_true, y_prob, y_hat):
    cm = confusion_matrix(y_true, y_hat)
    tn, fp, fn, tp = cm.ravel()
    spec = tn / (tn + fp)
    print(f"{lbl:5s} ROC={roc_auc_score(y_true, y_prob):.3f}  "
          f"PR={average_precision_score(y_true, y_prob):.3f}  "
          f"F1={f1_score(y_true, y_hat):.3f}  "
          f"Rec={recall_score(y_true, y_hat):.3f}  "
          f"Spec={spec:.3f}  "
          f"BalAcc={balanced_accuracy_score(y_true, y_hat):.3f}")

print("\nTest-set metrics (threshold 0.50):")
show_metrics("LR",   y_te, y_lr_prob,   y_lr_pred)
show_metrics("Tree", y_te, y_tree_prob, y_tree_pred)

print("\nSaved: cv_results_*, best_*.pkl, sampler_grid_*, "
      "y_prob_*.npy, y_pred_*.npy")






# ## Feature Scaling
# =======================================================================
# ## Feature Scaling  – final (robust-tol + fast Shapiro + OHE-version fix)
# =======================================================================
# Outputs created in the working directory
#   • prep_lr   – ColumnTransformer for Logistic-Regression pipeline
#   • prep_tree – ColumnTransformer for Decision-Tree pipeline
#   • skew_profile.csv, skew_hist.png
#   • artefacts/scaler_registry.json   (column lists, SHA-256 hashed)
# =======================================================================

import pathlib, hashlib, warnings, json, random, sklearn
import numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import shapiro
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import (
    StandardScaler, RobustScaler, PowerTransformer, OneHotEncoder
)

# -----------------------------------------------------------------------
# 0 ▸ Preconditions
# -----------------------------------------------------------------------
if "df" not in globals():
    raise NameError("DataFrame `df` not found.")
if "loan_status" not in df.columns:
    raise KeyError("Target column `loan_status` missing.")

epsilon          = 1e-12
ROBUST_IQR_TOL   = 0.02    # ±2 % relative tolerance for IQR≈1
SHAPIRO_MAX_ROWS = 5_000   # per-column sampling cap for Shapiro-Wilk

# OneHotEncoder keyword changes in scikit-learn ≥1.4
_OHE_KW = {"sparse_output": False} if sklearn.__version__ >= "1.4" else {"sparse": False}

# -----------------------------------------------------------------------
# 1 ▸ Numeric-column profiling  (exclude pure 0/1 flags)
# -----------------------------------------------------------------------
def _is_binary(series):
    u = series.dropna().unique()
    return len(u) <= 2 and set(u) <= {0, 1}

num_cols = [
    c for c in df.columns
    if df[c].dtype.kind in "if" and c != "loan_status" and not _is_binary(df[c])
]

profile = []
for col in num_cols:
    s         = df[col].dropna()
    p5, p95   = s.quantile([.05, .95])
    p1, p99   = s.quantile([.01, .99])
    iqr       = s.quantile(.75) - s.quantile(.25)
    profile.append(dict(
        feature  = col,
        skew     = s.skew(),
        IQR      = iqr,
        p95_p5   = abs(p95) / max(abs(p5),  epsilon),
        p99_p1   = abs(p99) / max(abs(p1),  epsilon),
        out_pct  = (s.sub(s.mean()).abs() > 3*s.std(ddof=0)).mean()*100,
        n_unique = s.nunique(),
        IQR_zero = iqr < epsilon
    ))

prof_df = pd.DataFrame(profile).sort_values("feature")
prof_df["heavy_tail"] = (prof_df["skew"].abs() > 2) & (prof_df["p95_p5"] > 20)

heavy_pct = 100 * prof_df["heavy_tail"].mean()
if heavy_pct > 30:
    gamma_thr = prof_df["skew"].abs().quantile(.70)
    prof_df["heavy_tail"] = (prof_df["skew"].abs() > gamma_thr) & (
                            prof_df["p95_p5"] > 20)
    warnings.warn(f"{heavy_pct:.1f}% heavy-tailed → adaptive γ = {gamma_thr:.2f}",
                  RuntimeWarning)

prof_df.to_csv("skew_profile.csv", index=False)
plt.figure(figsize=(5,3))
sns.histplot(np.abs(prof_df["skew"]), bins=20, color="steelblue")
plt.title("|skew| distribution"); plt.xlabel("|skew|"); plt.ylabel("count")
plt.tight_layout(); plt.savefig("skew_hist.png", dpi=300); plt.close()

print(f"📝  skew_profile.csv ({len(prof_df)} vars)  &  skew_hist.png saved")

# -----------------------------------------------------------------------
# 2 ▸ Decision matrix ⇒ std / robust / power columns
# -----------------------------------------------------------------------
pow_cols  = prof_df.loc[prof_df["p99_p1"] > 1_000, "feature"].tolist()
rob_cols  = prof_df.loc[prof_df["heavy_tail"],    "feature"].tolist()
std_cols  = [c for c in num_cols if c not in pow_cols + rob_cols]

iqr_zero = prof_df.set_index("feature")["IQR_zero"].to_dict()
rob_cols = [c for c in rob_cols if not iqr_zero[c]]
std_cols = [c for c in std_cols if not iqr_zero[c]]

# -----------------------------------------------------------------------
# 3 ▸ Categorical & boolean discovery
# -----------------------------------------------------------------------
cat_cols  = df.select_dtypes(["object","category"]).columns.tolist()
bool_cols = [
    c for c in df.columns
    if df[c].dtype == "bool" or (df[c].dtype == "uint8"
                                 and set(df[c].dropna().unique()) <= {0,1})
]
cat_cols = [c for c in cat_cols if c not in bool_cols]

# -----------------------------------------------------------------------
# 4 ▸ Helper: numeric block with imputer + scaler
# -----------------------------------------------------------------------
def _num_block(cols, scaler):
    return Pipeline([("imp", SimpleImputer(strategy="median")),
                     ("sc", scaler)]), cols

# -----------------------------------------------------------------------
# 5 ▸ ColumnTransformers
# -----------------------------------------------------------------------
def make_preprocessor(include_cont: bool) -> ColumnTransformer:
    trfs = []
    if include_cont:
        if std_cols:
            trfs.append(("std", *_num_block(std_cols, StandardScaler())))
        if rob_cols:
            trfs.append(("rob", *_num_block(rob_cols, RobustScaler())))
        if pow_cols:
            trfs.append(("pow", *_num_block(
                pow_cols, PowerTransformer(method="yeo-johnson",
                                           standardize=True))))
    else:  # Decision-Tree: raw numerics
        if num_cols:
            trfs.append(("num", "passthrough", num_cols))
    if cat_cols:
        trfs.append(("cat", OneHotEncoder(handle_unknown="ignore",
                                          **_OHE_KW), cat_cols))
    if bool_cols:
        trfs.append(("bool","passthrough", bool_cols))
    return ColumnTransformer(trfs, remainder="drop")

prep_lr   = make_preprocessor(include_cont=True)
prep_tree = make_preprocessor(include_cont=False)

# -----------------------------------------------------------------------
# 6 ▸ Validation – NaN / Inf / scale tests
# -----------------------------------------------------------------------
X_all = df.drop(columns="loan_status")
prep_lr.fit(X_all);   prep_tree.fit(X_all)

def _prefix(label:str) -> str:
    return label.split("__",1)[0] if "__" in label else ""

def _scaled_matrix(prep):
    X = prep.transform(X_all)
    try:
        names = prep.get_feature_names_out()
    except AttributeError:
        names = np.arange(X.shape[1]).astype(str)
    return X, np.asarray(names)

def _check_mu_sigma(mat, idx, tol_mu=1e-3, tol_sd=1e-2):
    mu = mat[:, idx].mean(0); sd = mat[:, idx].std(0, ddof=0)
    return (np.abs(mu) < tol_mu).all() and (np.abs(sd-1) < tol_sd).all()

def validate_prep(prep, name, check_scale=True):
    Xs, names = _scaled_matrix(prep)
    if np.isnan(Xs).any() or np.isinf(Xs).any():
        raise ValueError(f"{name}: NaN/Inf produced by scaling.")

    if check_scale:
        prefixes = np.array([_prefix(n) for n in names])

        # Standard-scaled
        idx_std = np.where(prefixes == "std")[0]
        if idx_std.size and not _check_mu_sigma(Xs, idx_std):
            raise ValueError(f"{name}: std cols not μ≈0, σ≈1.")

        # Robust-scaled  (median≈0, IQR≈1 ±2 %)
        idx_rob = np.where(prefixes == "rob")[0]
        if idx_rob.size:
            med = np.median(Xs[:, idx_rob], axis=0)
            iqr = np.percentile(Xs[:, idx_rob],75,axis=0) - \
                  np.percentile(Xs[:, idx_rob],25,axis=0)
            if (np.abs(med) > 1e-3).any() or (np.abs(iqr-1) > ROBUST_IQR_TOL).any():
                raise ValueError(f"{name}: robust cols not median≈0, IQR≈1±{ROBUST_IQR_TOL}.")

        # Power-scaled (μ≈0, σ≈1) + normality check
        idx_pow = np.where(prefixes == "pow")[0]
        if idx_pow.size and not _check_mu_sigma(Xs, idx_pow):
            raise ValueError(f"{name}: power cols not μ≈0, σ≈1.")
        if idx_pow.size:
            pvals = []
            for i in idx_pow:
                col = Xs[:, i]
                if col.size > SHAPIRO_MAX_ROWS:
                    col = col[random.sample(range(col.size), SHAPIRO_MAX_ROWS)]
                pvals.append(shapiro(col)[1] if col.size >= 3 else 1.0)
            print(f"   PowerT median Shapiro-p = {np.median(pvals):.3f}")

    print(f"✅ {name} validated – shape {Xs.shape}")

validate_prep(prep_lr,   "prep_lr",   check_scale=True)
validate_prep(prep_tree, "prep_tree", check_scale=False)

# -----------------------------------------------------------------------
# 7 ▸ Artefacts registry + hashes
# -----------------------------------------------------------------------
art = pathlib.Path("artefacts"); art.mkdir(exist_ok=True)
registry = {
    "std_cols"  : std_cols,
    "robust_cols": rob_cols,
    "power_cols": pow_cols,
    "cat_cols"  : cat_cols,
    "bool_cols" : bool_cols
}
json.dump(registry, open(art/"scaler_registry.json","w"), indent=2)

for fn in ("skew_profile.csv", "skew_hist.png", art/"scaler_registry.json"):
    digest = hashlib.sha256(open(fn,"rb").read()).hexdigest()[:12]
    print(f"🔒 {pathlib.Path(fn).name:25s}  SHA-256 = {digest}")

print(f"\n🎯  Feature Scaling complete: "
      f"{len(std_cols)+len(rob_cols)+len(pow_cols)} continuous vars scaled, "
      f"{len(cat_cols)} categoricals encoded; NaNs imputed, robust IQR "
      f"±{int(ROBUST_IQR_TOL*100)} % tolerance, Shapiro sampled ≤{SHAPIRO_MAX_ROWS} rows.")



# ## Logistic Regression Classifier
# ================================================================
# ## Logistic Regression Classifier  (FINAL • fast-run version)
# ================================================================
# Prerequisites from earlier notebook cells:
#   • prep_lr               – ColumnTransformer
#   • X_tr, y_tr, X_te, y_te – train/test split
# ================================================================

# 0 ── Imports ───────────────────────────────────────────────────
import sys, os, json, warnings, joblib, hashlib, datetime
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from imblearn.pipeline          import Pipeline          # ← imblearn pipeline
from sklearn.model_selection    import RepeatedStratifiedKFold, GridSearchCV, ParameterGrid
from sklearn.linear_model       import LogisticRegression
from sklearn.calibration        import CalibratedClassifierCV, calibration_curve
from sklearn.metrics            import (
    roc_auc_score, average_precision_score, brier_score_loss,
    confusion_matrix, balanced_accuracy_score, recall_score,
    f1_score, roc_curve
)
from imblearn.over_sampling     import SMOTE, SMOTENC, RandomOverSampler
from imblearn.combine           import SMOTETomek

SEED = 0
ART  = Path("artefacts"); ART.mkdir(exist_ok=True, parents=True)

# 1 ── Sampler discovery ─────────────────────────────────────────
try:
    origin_cats = json.load(open(ART / "scaler_registry.json"))["cat_cols"]
except (FileNotFoundError, KeyError):
    warnings.warn("scaler_registry.json missing – assuming no categoricals", RuntimeWarning)
    origin_cats = []

cat_mask = [i for i, c in enumerate(X_tr.columns) if c in origin_cats]
smote_nc = SMOTENC(categorical_features=cat_mask, random_state=SEED) if cat_mask else SMOTE(random_state=SEED)

base_samplers = list(dict.fromkeys([
    "passthrough", smote_nc, SMOTE(random_state=SEED),
]))  # keep only three to speed up

# 2 ── Pipeline skeleton (imblearn) ──────────────────────────────
pipe_lr = Pipeline([
    ("prep",    prep_lr),
    ("sampler", "passthrough"),            # overridden by GridSearch
    ("clf",     LogisticRegression(max_iter=4000, random_state=SEED))
])

# 3 ── Hyper-parameter grid (arrays → lists)  ────────────────────
param_grid_lr = [
    {   # liblinear (L1/L2)
        "sampler": base_samplers.copy(),
        "clf__solver": ["liblinear"],
        "clf__penalty": ["l1", "l2"],
        "clf__C": np.logspace(-3, 1, 5).tolist(),      # 5 values
        "clf__class_weight": ["balanced", None],
    },
    {   # saga (elastic-net)
        "sampler": base_samplers.copy(),
        "clf__solver": ["saga"],
        "clf__penalty": ["elasticnet"],
        "clf__l1_ratio": [0.3, 0.7],
        "clf__C": np.logspace(-3, 1, 4).tolist(),      # 4 values
        "clf__class_weight": [None],
    },
]

# 4 ── Compute-budget guard (≤60 param sets & ≤5 % n_train) ─────
cap = min(60, int(0.05 * len(X_tr)))          # ~360 fits with 6-fold CV
while True:
    total = len(ParameterGrid(param_grid_lr))
    if total <= cap:
        break
    warnings.warn(f"Grid {total} fits > cap {cap} – trimming", RuntimeWarning)

    # 4.1 drop largest C per block
    for blk in param_grid_lr:
        Cs = blk["clf__C"]
        if len(Cs) > 1 and total > cap:
            Cs.pop()
    total = len(ParameterGrid(param_grid_lr))

    # 4.2 drop sampler variants if still too large
    if total > cap:
        for blk in param_grid_lr:
            s = blk["sampler"]
            if len(s) > 1 and total > cap:
                s.pop()
                total = len(ParameterGrid(param_grid_lr))

    if total > cap:
        raise RuntimeError("Grid still exceeds compute cap – thin manually.")

# 5 ── Grid search (6-fold CV, ROC-AUC primary) ─────────────────
cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=2, random_state=SEED)  # 6 folds
gs = GridSearchCV(
    pipe_lr,
    param_grid_lr,
    cv=cv,
    scoring={"roc_auc": "roc_auc", "pr_auc": "average_precision"},
    refit="roc_auc",
    n_jobs=-1,
    verbose=1,
)
gs.fit(X_tr, y_tr)

# 6 ── Persist best model & CV results ───────────────────────────
today = datetime.date.today().isoformat()
best_path = ART / f"best_lr_{today}.pkl"
joblib.dump(gs.best_estimator_, best_path)
pd.DataFrame(gs.cv_results_).to_csv(ART / f"cv_results_lr_{today}.csv", index=False)
print(f"✔ Best CV ROC-AUC = {gs.best_score_:.4f}")

# 7 ── Test-set metrics (threshold 0.50) ─────────────────────────
best_lr = gs.best_estimator_
y_prob  = best_lr.predict_proba(X_te)[:, 1]
y_pred  = (y_prob >= 0.50)

metrics = {
    "ROC-AUC":  roc_auc_score(y_te, y_prob),
    "PR-AUC" :  average_precision_score(y_te, y_prob),
    "Brier"  :  brier_score_loss(y_te, y_prob),
    "F1"     :  f1_score(y_te, y_pred),
    "Recall" :  recall_score(y_te, y_pred),
    "Bal-Acc":  balanced_accuracy_score(y_te, y_pred),
}
for k, v in metrics.items():
    print(f"{k:8s}: {v:.3f}")

# 8 ── Calibration & isotonic adjustment ─────────────────────────
prev = y_te.mean()
if metrics["Brier"] > prev * (1 - prev) + 0.02:
    print("↪ Isotonic calibration applied")
    best_lr = CalibratedClassifierCV(best_lr, method="isotonic", cv="prefit")
    best_lr.fit(X_tr, y_tr)
    y_prob = best_lr.predict_proba(X_te)[:, 1]
    metrics["Brier.Cal"] = brier_score_loss(y_te, y_prob)
    print(f"Brier.Cal: {metrics['Brier.Cal']:.3f}")

# save calibration curve
prob_true, prob_pred = calibration_curve(y_te, y_prob, n_bins=10)
plt.figure(figsize=(3,3))
plt.plot(prob_pred, prob_true, "o-")
plt.plot([0,1], [0,1], "--", lw=0.8)
plt.xlabel("Predicted P"); plt.ylabel("Observed P")
plt.tight_layout()
plt.savefig(ART / "calibration_lr.png", dpi=300)
plt.close()

# 9 ── Interpretability: odds ratios ─────────────────────────────
inner_pipe = best_lr.base_estimator_ if isinstance(best_lr, CalibratedClassifierCV) else best_lr
clf   = inner_pipe.named_steps["clf"]
coef  = clf.coef_.ravel()
names = inner_pipe.named_steps["prep"].get_feature_names_out()
or_df = (pd.DataFrame({"feature": names, "beta": coef})
           .assign(OR=lambda d: np.exp(d["beta"]))
           .loc[lambda d: d["beta"] != 0]
           .sort_values("beta", key=np.abs, ascending=False)
           .head(10))

if not or_df.empty:
    plt.figure(figsize=(5,4))
    plt.barh(or_df["feature"], or_df["OR"])
    plt.axvline(1, color="k", lw=0.8)
    plt.xlabel("Odds ratio (exp β̂)")
    plt.title("Top ±10 predictors")
    plt.tight_layout()
    plt.savefig(ART / "oddsratio_lr.png", dpi=300)
    plt.close()
    or_df.to_csv(ART / "oddsratio_table_lr.csv", index=False)
else:
    print("No non-zero coefficients to plot.")

# 10 ── Threshold selection (Youden-J) ───────────────────────────
fpr, tpr, thr = roc_curve(y_te, y_prob, pos_label=1)
DEPLOY_THR    = thr[np.argmax(tpr - fpr)]

def _conf(label, yhat):
    cm = confusion_matrix(y_te, yhat, labels=[0,1])
    tn, fp, fn, tp = cm.ravel()
    print(f"\n{label}")
    print(cm)
    print(f"  Recall  : {tp/(tp+fn):.3f}")
    print(f"  Bal-Acc : {balanced_accuracy_score(y_te, yhat):.3f}")
    print(f"  F1      : {f1_score(y_te, yhat):.3f}")

_conf("Default p=0.50", y_pred)
_conf(f"Youden-J p={DEPLOY_THR:.2f}", (y_prob >= DEPLOY_THR))

# 11 ── Fairness audit (four-fifths rule) ───────────────────────
test_df = pd.concat([X_te, y_te.rename("loan_status")], axis=1)

def _group_metrics(col):
    grp = test_df[col].astype(str).fillna("NA")
    rows = []
    for g in grp.unique():
        mask  = grp == g
        n_pos = int(y_te[mask].sum())
        n_neg = int(mask.sum() - n_pos)
        yhat  = (y_prob[mask] >= DEPLOY_THR).astype(int)
        tn, fp, fn, tp = confusion_matrix(y_te[mask], yhat, labels=[0,1]).ravel()
        tpr = tp / (tp + fn) if (tp + fn) else 0
        rows.append((g, n_pos, n_neg, tpr))
    return pd.DataFrame(rows, columns=["group","n_pos","n_neg","TPR"])

for col in ["gender", "married", "property_area"]:
    if col in test_df.columns:
        tab = _group_metrics(col)
        best, worst = tab["TPR"].max(), tab["TPR"].min()
        ratio = worst / best if best else 0
        flag  = "⚠" if ratio < 0.80 or best == 0 else "✓"
        print(f"\nFairness ({col}) {flag}  TPR ratio={ratio:.2f}  "
              f"{tab[['group','n_pos','n_neg']].to_dict('records')}")

# 12 ── Environment & seed log ───────────────────────────────────
freeze_path = ART / f"env_{today}.txt"
if os.system(f"pip freeze > {freeze_path}") != 0:
    warnings.warn("pip freeze failed", RuntimeWarning)

meta = {
    "timestamp_utc": datetime.datetime.utcnow().strftime("%Y-%m-%dT%H:%M:%SZ"),
    "python":   f"{sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}",
    "numpy":    np.__version__,
    "sklearn":  __import__('sklearn').__version__,
    "imblearn": __import__('imblearn').__version__,
    "seed":     SEED,
}
meta_path = ART / "run_meta.json"
json.dump(meta, open(meta_path, "w"), indent=2)

for p in (best_path, freeze_path, meta_path):
    h = hashlib.sha256(open(p, "rb").read()).hexdigest()[:12]
    print(f"🔒 {p.name:25s} SHA-256={h}")



# ### Model performance - Cross-Validation
# ===============================================================
# ### Model performance – Cross-Validation   (≈ 2-minute runtime)
# ===============================================================
# Artefacts written to /artefacts :
#   • artefacts/summary_metrics.csv
#   • artefacts/roc_pr_box.png
#   • artefacts/fairness_bar.png
#   • artefacts/README_artifacts.md
#   • artefacts/SHA256_manifest.txt
# Requires: pipe_lr, pipe_tree, X_tr, y_tr
# ===============================================================

import numpy as np, pandas as pd, matplotlib.pyplot as plt, scipy
from sklearn.model_selection import (GridSearchCV, RepeatedStratifiedKFold,
                                     StratifiedKFold, cross_validate)
from sklearn.metrics import (roc_auc_score, average_precision_score,
                             brier_score_loss, roc_curve)
from scipy.stats import wilcoxon
from hashlib import sha256
from pathlib import Path

# ── 0  Globals & sanity ────────────────────────────────────────
assert scipy.__version__ >= "1.10", "SciPy ≥ 1.10 required"
SEED = 0
ART  = Path("artefacts"); ART.mkdir(parents=True, exist_ok=True)

# ── 1  Scorers & lean grids (4 combos each) ────────────────────
SCORERS = {"roc_auc": "roc_auc",
           "pr_auc" : "average_precision",
           "brier"  : "neg_brier_score"}          # sign flipped later

param_grid_lr   = {"clf__C":[0.5, 1.5], "clf__penalty":["l1", "l2"]}
param_grid_tree = {"clf__max_depth":[None, 10],
                   "clf__min_samples_leaf":[1, 5]}

# ── 2  Splitters (5×3 outer, 3-fold inner) ─────────────────────
outer_cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=SEED)
inner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=SEED)

def build_outer_iter(y):
    """Return outer CV iterator; fallback to bootstrap if minority <10."""
    if y.value_counts().min() >= 10:
        return list(outer_cv.split(X_tr, y_tr))
    print("⚠ Minority class <10 — switching to 1 000 stratified bootstraps")
    rng = np.random.default_rng(SEED)
    idx_all = np.arange(len(y))
    splits = []
    for _ in range(1_000):
        train_idx = rng.choice(idx_all, len(y), replace=True)
        oob       = np.setdiff1d(idx_all, np.unique(train_idx))
        splits.append((train_idx, oob if oob.size else idx_all))
    return splits

outer_iter = build_outer_iter(y_tr)

# ── 3  Nested-CV executor (≈90 fits per model) ─────────────────
def nested_cv(pipe, grid, label):
    gs = GridSearchCV(pipe, grid, cv=inner_cv,
                      scoring=SCORERS, refit="roc_auc",
                      n_jobs=-1, verbose=0)
    cv = cross_validate(gs, X_tr, y_tr,
                        cv=outer_iter, scoring=SCORERS,
                        return_estimator=True, n_jobs=-1)
    cv["test_brier"] = -cv["test_brier"]            # flip sign
    print(f"{label:7s}  ROC μ={cv['test_roc_auc'].mean():.3f}  "
          f"PR μ={cv['test_pr_auc'].mean():.3f}")
    return cv

lr_cv   = nested_cv(pipe_lr , param_grid_lr  , "LogReg")
tree_cv = nested_cv(pipe_tree, param_grid_tree, "DecTree")

# ── 4  Fold table & summary CSV ────────────────────────────────
def folds_df(res, mdl):
    return pd.DataFrame({"roc_auc":res["test_roc_auc"],
                         "pr_auc" :res["test_pr_auc"],
                         "brier"  :res["test_brier"]}).assign(model=mdl)

folds   = pd.concat([folds_df(lr_cv,"LR"), folds_df(tree_cv,"DT")], ignore_index=True)
summary = folds.groupby("model").agg(["mean","std"]).round(3)
summary.to_csv(ART / "summary_metrics.csv")
print("✓ artefacts/summary_metrics.csv written")

# ── 5  Wilcoxon, Holm, Cliff δ ─────────────────────────────────
diff_auc = lr_cv["test_roc_auc"] - tree_cv["test_roc_auc"]
diff_pr  = lr_cv["test_pr_auc"]  - tree_cv["test_pr_auc"]
p_adj = min(1,
            min(wilcoxon(diff_auc).pvalue,
                wilcoxon(diff_pr ).pvalue) * 2)      # Holm (m=2)
cliff_d = np.sign(diff_auc).mean()                    # Cliff δ
print(f"Holm-adj p = {p_adj:.3f}   Cliff δ = {cliff_d:.2f} "
      "(δ≈0 negligible · 0.33 medium · 0.47 large)")

# ── 6  Youden-J thresholds & fairness (auto-choose column) ─────
def youden_thr(est, X, y):
    fpr, tpr, thr = roc_curve(y, est.predict_proba(X)[:,1])
    return thr[np.argmax(tpr - fpr)]

# priority list of potential group columns (post-engineering)
GROUP_CANDIDATES = ["loan_term_bin", "education", "self_employed"]
group_col = next((c for c in GROUP_CANDIDATES if c in X_tr.columns), None)
if group_col is None:
    group_col = "N/A"
    ratio_lr = ratio_dt = 1.0
    print("⚠ No suitable fairness column found — four-fifths ratio set to 1.0")
else:
    def four_fifths(est, X, y, col, thr):
        grp  = X[col].astype(str)
        yhat = est.predict_proba(X)[:,1] >= thr
        tprs = [((yhat[grp==g] & (y[grp==g]==1)).sum()) /
                max(1,(y[grp==g]==1).sum()) for g in grp.unique()]
        return min(tprs)/max(tprs) if len(tprs)>1 else 1.0
    thr_lr   = youden_thr(lr_cv  ["estimator"][0], X_tr, y_tr)
    thr_dt   = youden_thr(tree_cv["estimator"][0], X_tr, y_tr)
    ratio_lr = four_fifths(lr_cv  ["estimator"][0], X_tr, y_tr, group_col, thr_lr)
    ratio_dt = four_fifths(tree_cv["estimator"][0], X_tr, y_tr, group_col, thr_dt)

# ── 7  Figures ─ dual boxplot & fairness bar ───────────────────
plt.figure(figsize=(5,3))
plt.boxplot([folds.query("model=='LR'")["roc_auc"],
             folds.query("model=='DT'")["roc_auc"]],
            positions=[1,2], tick_labels=["LR","DT"])
plt.boxplot([folds.query("model=='LR'")["pr_auc"],
             folds.query("model=='DT'")["pr_auc"]],
            positions=[4,5], tick_labels=["LR","DT"])
plt.xticks([1.5,4.5], ["ROC-AUC","PR-AUC"])
plt.ylabel("Cross-validated score")
plt.tight_layout(); plt.savefig(ART / "roc_pr_box.png", dpi=300); plt.close()

plt.figure(figsize=(2.6,3))
plt.bar(["LR","DT"], [ratio_lr, ratio_dt], color=["tab:blue","tab:orange"])
plt.axhline(0.8, color='r', ls='--')
plt.ylim(0,1); plt.ylabel(f"4/5ths TPR ratio  ({group_col})")
plt.tight_layout(); plt.savefig(ART / "fairness_bar.png", dpi=300); plt.close()

print("✓ artefacts/roc_pr_box.png & artefacts/fairness_bar.png written")

# ── 8  Manifest & README ───────────────────────────────────────
def shasum(fp): return sha256(Path(fp).read_bytes()).hexdigest()[:12]

files = ["summary_metrics.csv", "roc_pr_box.png", "fairness_bar.png",
         "README_artifacts.md"]
(ART / "README_artifacts.md").write_text(
    f"- summary_metrics.csv : mean ± sd of ROC, PR, Brier for LR & DT\n"
    f"- roc_pr_box.png      : dual boxplot (ROC-AUC & PR-AUC)\n"
    f"- fairness_bar.png    : worst four-fifths ratio by '{group_col}'\n"
    f"- grid trimmed ≥80 % if a warning appeared above\n"
)
(ART / "SHA256_manifest.txt").write_text(
    "\n".join(f"{shasum(ART/f)}  artefacts/{f}" for f in files)
)
print("✓ artefacts/SHA256_manifest.txt written")



# ### Model Optimization - Hyperparameter Tuning
# ================================================================
#  Logistic-Regression  •  Hyper-parameter Tuning & Deployment
#  Google Colab — single, self-contained cell  (no omissions)
#  Total wall-time ≤ 120 s on a standard Colab CPU runtime
# ================================================================

# ─────────── 0. Imports & hard guards ───────────────────────────
import os, sys, time, json, hashlib, warnings, pathlib, subprocess, joblib
import numpy  as np
import pandas as pd
import matplotlib.pyplot as plt
from imblearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import (
    GridSearchCV, StratifiedKFold, RepeatedStratifiedKFold
)
from sklearn.metrics import roc_curve, brier_score_loss
from sklearn.calibration import calibration_curve
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)

# Required in-memory objects
for obj in ("prep_lr", "X_tr", "y_tr", "X_te", "y_te"):
    if obj not in globals():
        raise NameError(f"❌ `{obj}` missing – run preprocessing/split cells first.")

RAW_CSV = pathlib.Path("loan_approval_dataset.csv")
if not RAW_CSV.is_file():
    raise FileNotFoundError("❌ 'loan_approval_dataset.csv' not in working dir.")

ART = pathlib.Path("artefacts"); ART.mkdir(exist_ok=True)

# ─────────── 1. Helper utilities ───────────────────────────────
def sha256(path: os.PathLike) -> str:
    return hashlib.sha256(open(path, "rb").read()).hexdigest()

def save_folds(tag: str, cv):
    np.savez_compressed(
        ART / f"fold_indices_{tag}.npz",
        **{f"fold_{i}": idx.astype(np.int32)
           for i, (_, idx) in enumerate(cv.split(X_tr, y_tr))}
    )

def run_grid(cv, tag: str):
    gs = GridSearchCV(
        PIPE, PARAM_GRID, cv=cv,
        scoring={"roc_auc": "roc_auc", "pr_auc": "average_precision"},
        refit="roc_auc", n_jobs=-1, verbose=2, error_score=np.nan
    )
    t0 = time.perf_counter(); gs.fit(X_tr, y_tr); rt = time.perf_counter() - t0
    per_fit = rt / (len(PARAM_GRID) * cv.get_n_splits())
    np.save(ART / f"per_fit_time_{tag}.npy", np.array(per_fit))
    (pd.DataFrame(gs.cv_results_)
       .dropna(subset=["mean_test_roc_auc"])
       .to_csv(ART / f"cv_results_{tag}.csv", index=False))
    save_folds(tag, cv)
    print(f"✔ {tag:7s} | {rt:6.1f}s  (≈ {per_fit:.3f}s/fit)")
    return gs, rt

# ─────────── 2. Grid & pipeline definitions ────────────────────
C_VALS = [0.01, 0.1, 1, 10, 100]
PARAM_GRID = [
    {"clf__penalty": ["l1"], "clf__dual": [False],
     "clf__C": C_VALS, "clf__class_weight": [None, "balanced"]},
    {"clf__penalty": ["l2"], "clf__dual": [False, True],
     "clf__C": C_VALS, "clf__class_weight": [None, "balanced"]},
]

PIPE = Pipeline([
    ("prep", prep_lr),
    ("clf",  LogisticRegression(
                 solver="liblinear",
                 max_iter=1000,
                 tol=5e-4,
                 random_state=0))
])

# ─────────── 3. Five-fold grid pass ────────────────────────────
cv5 = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)
gs, rt5 = run_grid(cv5, "5fold")
BEST = gs.best_estimator_
upgrade = False

# ─────────── 4. Five×Two grid pass if <60 s ────────────────────
rt10 = 0.0
if rt5 < 60:
    cv10 = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=0)
    gs, rt10 = run_grid(cv10, "10fold")
    BEST = gs.best_estimator_
    upgrade = True

# ─────────── 5. High-C micro-grid (100/300/1000) ───────────────
rt_micro = 0.0
if BEST.named_steps["clf"].C == 100 and (rt5 + rt10) < 110:
    micro_cfg = {k: ([v] if not isinstance(v, list) else v)
                 for k, v in gs.best_params_.items()}
    micro_cfg["clf__C"] = [100, 300, 1000]
    micro = GridSearchCV(
        PIPE, [micro_cfg], cv=gs.cv,
        scoring="roc_auc", refit="roc_auc",
        n_jobs=-1, verbose=0, error_score=np.nan
    )
    t0 = time.perf_counter(); micro.fit(X_tr, y_tr)
    rt_micro = time.perf_counter() - t0
    np.save(ART / "per_fit_time_micro.npy",
            np.array(rt_micro / (3 * gs.cv.get_n_splits())))
    if micro.best_score_ > gs.best_score_ + 1e-4:
        BEST = micro.best_estimator_
        print("↑ micro-grid improved AUC with C > 100")

# ─────────── 6. Persist model & metadata ───────────────────────
MODEL_FP = ART / "best_lr.pkl"
joblib.dump(BEST, MODEL_FP)

# robust capture of weight setting
cw_setting = BEST.named_steps["clf"].get_params()["class_weight"]
json.dump(cw_setting, open(ART / "class_weights.json", "w"))

json.dump(upgrade, open(ART / "cv_upgrade_taken.json", "w"))
(ART / "cv_repr.txt").write_text(repr(gs.cv))
MODEL_SHA = sha256(MODEL_FP)

# ─────────── 7. TEST-set evaluation & plots ────────────────────
y_prob = BEST.predict_proba(X_te)[:, 1]
print(f"Brier score (TEST) = {brier_score_loss(y_te, y_prob):.3f}")

prob_true, prob_pred = calibration_curve(y_te, y_prob, n_bins=10)
plt.figure(figsize=(3,3))
plt.plot(prob_pred, prob_true, "o-"); plt.plot([0,1],[0,1],"--",lw=0.7)
plt.xlabel("Predicted probability"); plt.ylabel("Observed frequency")
plt.title("Calibration (TEST)"); plt.tight_layout()
plt.savefig(ART / "calibration_lr.png", dpi=300); plt.close()

coeff = BEST.named_steps["clf"].coef_.ravel()
try:
    feat_names = BEST.named_steps["prep"].get_feature_names_out()
except AttributeError:
    feat_names = np.arange(coeff.size).astype(str)
or_df = (pd.DataFrame({"feature": feat_names,
                       "OR": np.exp(coeff)})
           .assign(abs_beta=lambda d: np.abs(np.log(d.OR)))
           .sort_values("abs_beta", ascending=False)
           .head(10))
or_df.to_csv(ART / "oddsratio_top10_lr.csv", index=False)
plt.figure(figsize=(5,3.5))
plt.barh(or_df.feature, or_df.OR)
plt.axvline(1, color="k", lw=0.7)
plt.xlabel("Odds ratio"); plt.title("Top |β|"); plt.tight_layout()
plt.savefig(ART / "oddsratio_top10_lr.png", dpi=300); plt.close()

# ─────────── 8. Youden-J threshold & deploy-meta ───────────────
fpr, tpr, thr = roc_curve(y_te, y_prob)
youden_thr = float(thr[np.argmax(tpr - fpr)])
json.dump(
    {"timestamp_utc": time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()),
     "model_sha256" : MODEL_SHA,
     "youden_J_thr" : youden_thr},
    open(ART / "deploy_meta.json", "w"), indent=2
)

# ─────────── 9. pip freeze & raw-CSV hash -----------------------
FREEZE_FP = ART / "pip_freeze.txt"
with open(FREEZE_FP, "w") as fh:
    subprocess.run([sys.executable, "-m", "pip", "freeze"],
                   stdout=fh, check=False)
RAW_SHA = sha256(RAW_CSV)

# ─────────── 10. Manifest (hash of every artefact) -------------
manifest = {
    p.name: sha256(p)
    for p in ART.iterdir()
    if p.is_file() and not p.name.startswith('.')
}
manifest["loan_approval_dataset.csv"] = RAW_SHA
(ART / "manifest.txt").write_text(
    "\n".join(f"{h}  {n}" for n, h in sorted(manifest.items()))
)

# ─────────── 11. Final summary ---------------------------------
total_rt = rt5 + rt10 + rt_micro
print(f"\n🏁 Logistic-Regression tuning completed in {total_rt:,.1f} s")
print(f"   Model SHA-256 : {MODEL_SHA}")
print(f"   Youden-J thr  : {youden_thr:.3f}")
print(f"   Artefacts dir : {ART.resolve()}")



# ### Best tuned model
# ===============================================================
# ### Best tuned model  –  Logistic Regression
# Robust load ➜ summary ➜ odds-ratio artefacts
# No test-set access | runtime ≈ 1 s
# ===============================================================

from pathlib import Path
import joblib, json, warnings
import numpy as np, pandas as pd, matplotlib.pyplot as plt
from sklearn.calibration import CalibratedClassifierCV

ART = Path("artefacts")

# ── 1 │ Retrieve tuned pipeline (RAM → disk fallback) ──────────
best_lr = None
for _v in ("gs", "gs_lr"):
    if _v in globals():
        best_lr = globals()[_v].best_estimator_
        break

if best_lr is None:
    pkl_files = list(ART.glob("best_lr_*.pkl"))
    if not pkl_files:
        raise FileNotFoundError("❌ best_lr_*.pkl not found — run the tuning cell.")
    best_lr = joblib.load(max(pkl_files, key=lambda p: p.stat().st_mtime))

assert hasattr(best_lr, "predict_proba"), "❌ Loaded object lacks predict_proba"

inner = best_lr.base_estimator_ if isinstance(best_lr, CalibratedClassifierCV) else best_lr
prep, clf = inner.named_steps["prep"], inner.named_steps["clf"]

# ── 2 │ Hyper-parameters & CV AUC (RAM → CSV fallback) ─────────
try:
    gs_obj   = globals().get("gs") or globals().get("gs_lr")
    mean_auc = float(gs_obj.best_score_)
    std_auc  = float(gs_obj.cv_results_["std_test_roc_auc"][gs_obj.best_index_])
    params   = gs_obj.best_params_
except Exception:
    csv_fp = max(ART.glob("cv_results_lr_*.csv"), key=lambda p: p.stat().st_mtime)
    cv_row = pd.read_csv(csv_fp).query("rank_test_roc_auc == 1").iloc[0]
    mean_auc, std_auc = float(cv_row.mean_test_roc_auc), float(cv_row.std_test_roc_auc)
    params = {k.replace("param_", ""): cv_row[k] for k in cv_row.index if k.startswith("param_")}

clean_params = {k: (v.__class__.__name__ if k.startswith("sampler") else v)
                for k, v in params.items()}
print("Best hyper-parameters :", clean_params)
print(f"Mean CV ROC-AUC       : {mean_auc:.3f} ± {std_auc:.3f}")

# ── 3 │ Odds-ratio table & plot (idempotent) ───────────────────
names = prep.get_feature_names_out()
coef  = clf.coef_.toarray() if hasattr(clf.coef_, "toarray") else clf.coef_
beta  = coef.ravel() if coef.ndim == 2 else coef

if (beta != 0).sum() == 0:
    warnings.warn("All coefficients are zero — odds-ratio CSV saved, plot skipped.")
    odds_df = pd.DataFrame({"feature": names, "OR": np.exp(beta)})
else:
    odds_df = (pd.DataFrame({"feature": names, "beta": beta, "OR": np.exp(beta)})
                 .query("beta != 0")
                 .assign(abs_beta=lambda d: d.beta.abs())
                 .nlargest(10, "abs_beta"))

# sort so strongest (largest |β|) appears at top of barh plot
if "abs_beta" in odds_df.columns:
    odds_df = odds_df.sort_values("abs_beta").reset_index(drop=True)

csv_out, png_out = ART / "oddsratio_table_lr.csv", ART / "oddsratio_lr.png"
odds_df.to_csv(csv_out, index=False)                             # always overwrite for consistency

if (beta != 0).sum() and not png_out.is_file():
    plt.figure(figsize=(5, 4))
    plt.barh(odds_df.feature, odds_df.OR)
    plt.xscale("log")                                           # display OR<1 and OR>1 on same axis
    plt.axvline(1, color="k", lw=0.8)
    plt.xlabel("Odds ratio (exp β̂)")
    plt.title("Top |β| predictors")
    plt.annotate("log-scale: OR < 1 = protective",
                 xy=(0.97, 0.02), xycoords="figure fraction",
                 ha="right", fontsize=7)
    plt.tight_layout()
    plt.savefig(png_out, dpi=300)
    plt.close()

# ── 4 │ Audit trace (no training impact) ───────────────────────
meta = json.load(open(ART / "deploy_meta.json"))
print(f"Youden-J threshold (audit only): {meta['youden_J_thr']:.2f}")
print(f"Model SHA-256 (first 12 hex)   : {meta['model_sha256'][:12]}…")

# ── 5 │ Expose objects for downstream cells ────────────────────
globals().update(best_lr=best_lr, odds_df=odds_df)



# ### Model Evaluation on Test Set
# ===============================================================
#  📊  MODEL EVALUATION ON TEST SET — Logistic-Regression
#  (writes all outputs to artefacts/, runtime ≤ 2 min on Colab CPU)
# ===============================================================

# 0 ▸ Imports & globals
import os, time, json, hashlib, warnings
import numpy as np, pandas as pd
import matplotlib.pyplot as plt; import seaborn as sns
from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    balanced_accuracy_score, recall_score, f1_score,
    confusion_matrix, roc_curve, precision_recall_curve
)
from sklearn.calibration import calibration_curve

np.random.seed(0)
ART = "artefacts/"; os.makedirs(ART, exist_ok=True)
t0 = time.perf_counter()

# ---------------------------------------------------------------
# 1 ▸  Load test probabilities (ŷ) and labels (y)
# ---------------------------------------------------------------
y_prob = np.load("y_prob_lr.npy")                             # saved earlier

try:                                                          # fast path
    y_te = np.load(f"{ART}y_test.npy")
except FileNotFoundError:                                     # cold start
    raw = pd.read_csv("loan_approval_dataset.csv")

    # ► normalise headers
    raw.columns = (raw.columns
                     .str.strip()
                     .str.lower()
                     .str.replace(r"\s+", "_", regex=True))

    if "loan_status" not in raw.columns:
        raise KeyError("'loan_status' column not found "
                       f"(available columns: {list(raw.columns)})")

    raw["loan_status"] = (raw["loan_status"]
                          .astype(str)
                          .str.strip()
                          .str.lower()
                          .map({"approved": 1, "rejected": 0}))

    test_idx = np.load(f"{ART}split_idx.npz")["test"]
    y_te = raw.loc[test_idx, "loan_status"].to_numpy()
    np.save(f"{ART}y_test.npy", y_te)

youden_thr  = json.load(open(f"{ART}deploy_meta.json"))["youden_J_thr"]
thr_default = 0.50

# ---------------------------------------------------------------
# 2 ▸  Point-estimate metrics
# ---------------------------------------------------------------
stats = {}
for tag, thr in {"0.50": thr_default, "Youden": youden_thr}.items():
    y_hat = y_prob >= thr
    tn, fp, fn, tp = confusion_matrix(y_te, y_hat, labels=[0, 1]).ravel()
    stats |= {
        f"Recall_{tag}":        recall_score(y_te, y_hat),
        f"Specificity_{tag}":   tn / (tn + fp),
        f"F1_{tag}":            f1_score(y_te, y_hat),
        f"BalAcc_{tag}":        balanced_accuracy_score(y_te, y_hat),
    }

stats |= {
    "ROC_AUC": roc_auc_score(y_te, y_prob),
    "PR_AUC":  average_precision_score(y_te, y_prob),
    "Brier":   brier_score_loss(y_te, y_prob),
}

# ---------------------------------------------------------------
# 3 ▸  Stratified bootstrap (1 000×) → 95 % CI & p-values
# ---------------------------------------------------------------
rng = np.random.default_rng(0)
idx_pos = np.where(y_te == 1)[0]
idx_neg = np.where(y_te == 0)[0]
roc_b, pr_b = [], []

for _ in range(1_000):
    boot = np.concatenate([
        rng.choice(idx_pos, size=idx_pos.size, replace=True),
        rng.choice(idx_neg, size=idx_neg.size, replace=True)
    ])
    roc_b.append(roc_auc_score(y_te[boot], y_prob[boot]))
    pr_b.append(average_precision_score(y_te[boot], y_prob[boot]))

roc_b, pr_b = map(np.asarray, (roc_b, pr_b))
ci = {
    "ROC_CI": tuple(np.percentile(roc_b, [2.5, 97.5])),
    "PR_CI":  tuple(np.percentile(pr_b, [2.5, 97.5])),
    "ROC_p":  2 * min((roc_b < 0.5).mean(), (roc_b > 0.5).mean()),
    "PR_p":   2 * min((pr_b < y_te.mean()).mean(),
                      (pr_b > y_te.mean()).mean()),
}
np.savez(f"{ART}boot_metrics_lr.npz", roc=roc_b, pr=pr_b)

# ---------------------------------------------------------------
# 4 ▸  Calibration curve + Murphy Brier split
# ---------------------------------------------------------------
pt, pp = calibration_curve(y_te, y_prob, n_bins=10)
rel = ((pp - pt) ** 2).mean()
res = ((pt - y_te.mean()) ** 2).mean()
unc = y_te.mean() * (1 - y_te.mean())

plt.figure(figsize=(3, 3))
plt.plot(pp, pt, "o-"); plt.plot([0, 1], [0, 1], "--", lw=.8)
plt.xlabel("Predicted probability"); plt.ylabel("Observed frequency")
plt.tight_layout(); plt.savefig(f"{ART}calibration_lr.png", dpi=300); plt.close()

# ---------------------------------------------------------------
# 5 ▸  ROC, PR, confusion-matrix plots
# ---------------------------------------------------------------
# ROC
fpr, tpr, thr = roc_curve(y_te, y_prob)
plt.figure(figsize=(3, 3))
plt.plot(fpr, tpr); plt.plot([0, 1], [0, 1], "--", lw=.8)
plt.scatter(fpr[np.argmin(np.abs(thr - youden_thr))],
            tpr[np.argmin(np.abs(thr - youden_thr))])
plt.xlabel("False-positive rate"); plt.ylabel("True-positive rate")
plt.tight_layout(); plt.savefig(f"{ART}roc_lr.png", dpi=300); plt.close()

# PR
prec, reca, thr_pr = precision_recall_curve(y_te, y_prob)
plt.figure(figsize=(3, 3))
plt.plot(reca, prec)
plt.axhline(y_te.mean(), xmin=0, xmax=1, color="gray", ls="--", lw=.8)  # baseline
plt.scatter(reca[np.argmin(np.abs(thr_pr - youden_thr))],
            prec[np.argmin(np.abs(thr_pr - youden_thr))])
plt.xlabel("Recall"); plt.ylabel("Precision")
plt.tight_layout(); plt.savefig(f"{ART}pr_lr.png", dpi=300); plt.close()

# Confusion matrix @ Youden-J
cm = confusion_matrix(y_te, y_prob >= youden_thr, labels=[0, 1])
plt.figure(figsize=(2.5, 2))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", cbar=False)
plt.ylabel("True label"); plt.xlabel("Predicted label")
plt.tight_layout(); plt.savefig(f"{ART}cm_lr.png", dpi=300); plt.close()

# ---------------------------------------------------------------
# 6 ▸  Fairness audit (4⁄5-ths rule) — if X_te present
# ---------------------------------------------------------------
if "X_te" in globals():
    for col in ("gender", "married", "property_area"):
        if col in X_te.columns:
            grp = X_te[col].astype(str).fillna("NA")
            tprs = [((y_prob[grp == g] >= youden_thr) &
                     (y_te[grp == g] == 1)).sum() /
                    max(1, (y_te[grp == g] == 1).sum())
                    for g in grp.unique()]
            max_tpr = max(tprs)
            stats["Fair_ratio"] = min(tprs) / max_tpr if max_tpr else np.nan
            plt.figure(figsize=(2.5, 3))
            plt.bar(grp.unique(), tprs); plt.axhline(0.8, ls="--")
            plt.ylabel("TPR"); plt.tight_layout()
            plt.savefig(f"{ART}fairness_lr.png", dpi=300); plt.close()
            break
else:
    warnings.warn("Fairness audit skipped: X_te unavailable", RuntimeWarning)

# ---------------------------------------------------------------
# 7 ▸  Persist metrics JSON
# ---------------------------------------------------------------
tn_050, fp_050, _, _ = confusion_matrix(y_te, y_prob >= thr_default, labels=[0,1]).ravel()
tn_y,   fp_y,   _, _ = confusion_matrix(y_te, y_prob >= youden_thr,  labels=[0,1]).ravel()
stats["Specificity_0.50"] = tn_050 / (tn_050 + fp_050)
stats["Specificity_Youden"] = tn_y / (tn_y + fp_y)
json.dump(
    dict(stats, **ci, Brier_split=dict(rel=rel, res=res, unc=unc)),
    open(f"{ART}test_metrics_lr.json", "w"), indent=2
)

# ---------------------------------------------------------------
# 8 ▸  SHA-256 manifest update
# ---------------------------------------------------------------
def _sha(fp): return hashlib.sha256(open(fp, "rb").read()).hexdigest()[:12]

files = [f"{ART}{f}" for f in (
    "test_metrics_lr.json", "boot_metrics_lr.npz", "calibration_lr.png",
    "roc_lr.png", "pr_lr.png", "cm_lr.png")]
if os.path.exists(f"{ART}fairness_lr.png"):
    files.append(f"{ART}fairness_lr.png")

with open(f"{ART}SHA256_manifest.txt", "a") as fh:
    for f in files:
        fh.write(f"{_sha(f)}  {f}\n")

# ---------------------------------------------------------------
# 9 ▸  Console summary
# ---------------------------------------------------------------
elapsed = time.perf_counter() - t0
print("\n=== TEST-SET SUMMARY ===")
for k in ("ROC_AUC", "PR_AUC", "Brier",
          "Recall_0.50", "Recall_Youden",
          "Specificity_0.50", "Specificity_Youden",
          "F1_0.50", "F1_Youden",
          "BalAcc_0.50", "BalAcc_Youden"):
    print(f"{k:16s}: {stats[k]:.3f}")

print(f"ROC 95 % CI: {np.round(ci['ROC_CI'],3)}  p = {ci['ROC_p']:.4f}")
print(f"PR  95 % CI: {np.round(ci['PR_CI'],3)}  p = {ci['PR_p']:.4f}")
if "Fair_ratio" in stats:
    print("4⁄5-ths TPR ratio:", f"{stats['Fair_ratio']:.2f}")
print(f"⏱ Completed in {elapsed:.1f} s")



# ## Decision Tree Classifier
# %% [markdown] ###############################################################
# ## Decision-Tree Classifier (CART)
# -----------------------------------------------------------------------------
#  • Leaves every Logistic-Regression artefact untouched
#  • Keeps original ExtraTrees CSV (copies it, does NOT rename)
#  • Writes all new CART artefacts into  `artefacts/`, plus legacy duplicates
#    `y_prob_tree.npy` / `y_pred_tree.npy` in the notebook root
#  • Appends SHA-256 hashes with `cart__` (and `extratree__` for the copy)
#  • Runs in ≈ 3 min on Colab CPU  — well under 6-minute section limit
# -----------------------------------------------------------------------------

# %% 1 ── Guards & lightweight imports ────────────────────────────────────────
SEED = 0
for var in ("prep_tree", "X_tr", "y_tr", "X_te", "y_te"):
    assert var in globals(), f"{var} missing — run previous cells."

import time, json, joblib, hashlib, numpy as np, pandas as pd
from pathlib import Path
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import (
    GridSearchCV, RepeatedStratifiedKFold,
    StratifiedKFold, cross_validate
)
from sklearn.base import clone

ART = Path("artefacts"); ART.mkdir(exist_ok=True)

# %% 2 ── Sampler grid (exactly two options) ─────────────────────────────────-
sampler_grid = (
    sampler_grid[:2] if "sampler_grid" in globals()
    else ["passthrough", SMOTE(random_state=SEED)]
)

# %% 3 ── Data-driven α_dyn (clone pre-processor to avoid leakage) ───────────
prep_temp = clone(prep_tree)
Xt = prep_temp.fit_transform(X_tr)

path = DecisionTreeClassifier().cost_complexity_pruning_path(Xt, y_tr)
alpha_list = path.ccp_alphas                     # correct field name
alpha_dyn  = min(0.002,
                 np.median(alpha_list[:3]) if len(alpha_list) >= 3 else 5e-4)

# %% 4 ── Pipeline & parameter grid ─────────────────────────────────────────-
pipe_cart = Pipeline([
    ("prep",    clone(prep_tree)),               # unfitted clone
    ("sampler", "passthrough"),
    ("clf",     DecisionTreeClassifier(random_state=SEED)),
])

param_grid_cart = {
    "sampler":               sampler_grid,
    "clf__max_depth":        [None, 12],
    "clf__min_samples_leaf": [1, 5],
    "clf__ccp_alpha":        [0.0, alpha_dyn],
    "clf__class_weight":     [None, "balanced"],
}

cv10 = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED)

# %% 5 ── GridSearchCV (320 fits) with watchdog ─────────────────────────────-
gs_cart = GridSearchCV(
    pipe_cart, param_grid_cart, cv=cv10,
    scoring={"roc_auc": "roc_auc", "average_precision": "average_precision"},
    refit="roc_auc", n_jobs=-1, verbose=0
)

t0 = time.perf_counter()
gs_cart.fit(X_tr, y_tr)
elapsed = time.perf_counter() - t0
print(f"CART GridSearch 320 fits completed in {elapsed:.1f}s")
if elapsed > 230:
    raise TimeoutError("CART grid exceeded 230-second safety limit")

# %% 6 ── Quick 5-fold CV → tree_cv (for downstream placeholders) ────────────
tree_cv = cross_validate(
    gs_cart.best_estimator_, X_tr, y_tr,
    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED),
    scoring="roc_auc", return_train_score=False
)

# %% 7 ── Predictions on test set + legacy duplicates ------------------------
best_cart  = gs_cart.best_estimator_
best_tree  = best_cart                    # overwrite for downstream cells
best_model = best_cart

y_cart_prob = best_cart.predict_proba(X_te)[:, 1]
y_cart_pred = (y_cart_prob >= 0.50).astype(int)

# CART-specific arrays
np_prob_cart = ART / "y_prob_tree_cart.npy"
np_pred_cart = ART / "y_pred_tree_cart.npy"
np.save(np_prob_cart, y_cart_prob)
np.save(np_pred_cart, y_cart_pred)

# Legacy filenames (root) — overwrite ExtraTrees artefact
np.save("y_prob_tree.npy", y_cart_prob)
np.save("y_pred_tree.npy", y_cart_pred)

# %% 8 ── Save grid CSV, model pickle, sampler JSON --------------------------
today = pd.Timestamp.utcnow().date().isoformat()

csv_cart_fp = ART / f"cv_results_tree_cart_{today}.csv"
pd.DataFrame(gs_cart.cv_results_).to_csv(csv_cart_fp, index=False)

pickle_fp = ART / f"best_cart_{today}.pkl"
joblib.dump(best_cart, pickle_fp)

# JSON-safe sampler list
sampler_grid_json = [
    s if isinstance(s, str) else s.__class__.__name__ for s in sampler_grid
]
json_fp = ART / "sampler_grid_cart.json"
json.dump(sampler_grid_json, open(json_fp, "w"), indent=2)

# %% 9 ── Copy (not rename) original ExtraTrees CSV --------------------------
old_tree_csv = max(Path(".").glob("cv_results_tree_*.csv"))
copy_csv = ART / f"{old_tree_csv.stem}_extratree.csv"
if not copy_csv.exists():
    copy_csv.write_bytes(old_tree_csv.read_bytes())

# %% 10 ── Append hashes to manifest ----------------------------------------
def sha(fp: Path) -> str:
    return hashlib.sha256(fp.read_bytes()).hexdigest()[:12]

new_files = [
    pickle_fp, csv_cart_fp, copy_csv,
    np_prob_cart, np_pred_cart,
    Path("y_prob_tree.npy"), Path("y_pred_tree.npy"),
    json_fp
]

with open(ART / "SHA256_manifest.txt", "a") as fh:
    for fp in new_files:
        prefix = "extratree__" if fp is copy_csv else "cart__"
        fh.write(f"{prefix}{sha(fp)}  {fp.relative_to(Path('.'))}\n")

# %% 11 ── Diagnostics -------------------------------------------------------
clf = best_cart.named_steps["clf"]
print(f"Best ROC-AUC : {gs_cart.best_score_:.4f}")
print(f"Tree depth   : {clf.get_depth()}  |  Leaves : {clf.get_n_leaves()}")
print("New artefacts :", ", ".join(p.name for p in new_files))



# ### Model performance - Cross-Validation
# ==============================================================
# ### Model performance – Cross-Validation (Decision-Tree CART)
#     • 15-fold RepeatedStratifiedKFold (5×3)
#     • Same metrics as Logistic-Regression + balanced accuracy
#     • Outputs: folds_cart.csv, summary_metrics_cart.csv,
#                roc_pr_box_cart.png, fairness_bar_cart.png (opt)
# ==============================================================

# ── 0. Guards ─────────────────────────────────────────────────
assert 'best_cart' in globals() or 'best_tree' in globals(), \
       "Run the Decision-Tree tuning cell first."
best_cart = globals().get('best_cart', globals()['best_tree'])

assert {'X_tr', 'y_tr', 'SEED'}.issubset(globals()), \
       "Run the data-splitting cell first."

# ── 1. Imports ────────────────────────────────────────────────
import time, pathlib, json, hashlib
import numpy as np, pandas as pd, matplotlib.pyplot as plt

from sklearn.model_selection import RepeatedStratifiedKFold, cross_validate
from sklearn.metrics import (
    make_scorer, balanced_accuracy_score, brier_score_loss
)
from scipy.stats import wilcoxon

# ── 2. CV splitter & scorers ─────────────────────────────────
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=SEED)

SCORERS = {
    "roc_auc" : "roc_auc",
    "pr_auc"  : "average_precision",
    "brier"   : make_scorer(brier_score_loss,
                            needs_proba=True,
                            greater_is_better=False),
    "bal_acc" : make_scorer(balanced_accuracy_score),
}

# ── 3. Run cross-validation ──────────────────────────────────
t0 = time.perf_counter()

cv_res = cross_validate(
    best_cart, X_tr, y_tr,
    cv=cv, scoring=SCORERS,
    return_estimator=False,
    n_jobs=-1, verbose=0)

if (time.perf_counter() - t0) > 280:
    raise TimeoutError("CART CV exceeded 280-s time budget")

cv_res["test_brier"] = -cv_res["test_brier"]          # make Brier positive

# ── 4. Fold table & summary ─────────────────────────────────
ART = pathlib.Path("artefacts"); ART.mkdir(exist_ok=True)

folds_cart = pd.DataFrame({
    "roc_auc": cv_res["test_roc_auc"],
    "pr_auc" : cv_res["test_pr_auc"],
    "brier"  : cv_res["test_brier"],
    "bal_acc": cv_res["test_bal_acc"],
})
folds_cart.to_csv(ART / "folds_cart.csv", index=False)

summary_metrics_cart = folds_cart.agg(["mean", "std"]).round(3)

# ── 5. Paired Wilcoxon vs. Logistic-Regression (optional) ───
if ("folds" in globals()
    and len(folds_cart) == len(folds.query("model=='LR'"))):

    cliff = np.sign(
        folds_cart.roc_auc - folds.query("model=='LR'").roc_auc
    ).mean()

    try:
        p_auc = wilcoxon(
            folds.query("model=='LR'").roc_auc,
            folds_cart.roc_auc
        ).pvalue
    except ValueError:          # all diffs zero
        p_auc = 1.0

    summary_metrics_cart.loc["stat", ["roc_auc", "pr_auc"]] = [p_auc, cliff]

summary_metrics_cart = summary_metrics_cart.round(3)
summary_metrics_cart.to_csv(ART / "summary_metrics_cart.csv")

# ── 6. Dual box-plot ─────────────────────────────────────────
plt.figure(figsize=(5, 3))
plt.boxplot(
    [folds_cart.roc_auc, folds_cart.pr_auc],
    positions=[1, 2], labels=["ROC-AUC", "PR-AUC"]
)
plt.ylabel("Cross-validated score")
plt.tight_layout()
plt.savefig(ART / "roc_pr_box_cart.png", dpi=300)
plt.close()

# ── 7. Fairness (4⁄5 TPR ratio on train) ─────────────────────
GROUP_CANDS = ["loan_term_bin", "education", "self_employed"]
group_col   = next((c for c in GROUP_CANDS if c in X_tr.columns), None)

if group_col is not None:
    grp   = X_tr[group_col].astype(str).fillna("NA")
    y_hat = best_cart.predict(X_tr)
    tprs  = []
    for g in grp.unique():
        mask = grp == g
        tp   = ((y_tr[mask]==1) & (y_hat[mask]==1)).sum()
        fn   = ((y_tr[mask]==1) & (y_hat[mask]==0)).sum()
        tprs.append(tp / max(1, tp+fn))
    ratio = min(tprs) / max(tprs) if len(tprs) > 1 else 1.0

    plt.figure(figsize=(2.5, 3))
    plt.bar(["CART"], [ratio]); plt.axhline(0.8, ls="--")
    plt.ylabel("4⁄5 TPR ratio (train)")
    plt.tight_layout()
    plt.savefig(ART / "fairness_bar_cart.png", dpi=300)
    plt.close()

# ── 8. Manifest update ───────────────────────────────────────
def _sha(fp): return hashlib.sha256(open(fp, "rb").read()).hexdigest()[:12]

new_files = [
    "folds_cart.csv",
    "summary_metrics_cart.csv",
    "roc_pr_box_cart.png",
]
if (ART / "fairness_bar_cart.png").exists():
    new_files.append("fairness_bar_cart.png")

with open(ART / "SHA256_manifest.txt", "a") as fh:
    for f in new_files:
        fh.write(f"cart__{_sha(ART/f)}  artefacts/{f}\n")

# ── 9. Console summary ───────────────────────────────────────
print("\nCART 15-fold CV – mean ± sd (stat row if present):")
print(summary_metrics_cart)



# ### Model Optimization - Hyperparameter Tuning
# ================================
# Decision-Tree Hyperparameter Tuning (CART)
# Entire section must finish within 6 minutes on Colab
# ================================

import time
from pathlib import Path
import numpy as np
import pandas as pd
import joblib
import json
import matplotlib.pyplot as plt
from sklearn.model_selection import RepeatedStratifiedKFold, GridSearchCV, cross_validate
from sklearn.tree import DecisionTreeClassifier
from imblearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE
from scipy.stats import wilcoxon
from hashlib import sha256

# 0. Section-wide timer
_section_start = time.perf_counter()

# Constants & paths
SEED = 0
ART = Path("artefacts"); ART.mkdir(exist_ok=True)
today = pd.Timestamp.utcnow().date().isoformat()

# 1. Hyperparameter GridSearchCV (≈230 s)
pipe_cart = Pipeline([
    ("prep",    prep_tree),
    ("sampler", "passthrough"),
    ("clf",     DecisionTreeClassifier(random_state=SEED)),
])

param_grid_cart = {
    "sampler":               ["passthrough", SMOTE(random_state=SEED)],
    "clf__max_depth":        [None, 12],
    "clf__min_samples_leaf": [1, 5],
    "clf__ccp_alpha":        [0.0, alpha_dyn],
    "clf__class_weight":     [None, "balanced"],
}

cv10 = RepeatedStratifiedKFold(n_splits=5, n_repeats=2, random_state=SEED)
gs_cart = GridSearchCV(
    pipe_cart, param_grid_cart,
    cv=cv10,
    scoring={"roc_auc":"roc_auc", "average_precision":"average_precision"},
    refit="roc_auc",
    n_jobs=-1,
)

t0 = time.perf_counter()
gs_cart.fit(X_tr, y_tr)
if time.perf_counter() - t0 > 230:
    raise TimeoutError("Grid-search exceeded 230 seconds")

# 2. Save Model, CV Results & Predictions
joblib.dump(gs_cart.best_estimator_, ART/f"best_cart_{today}.pkl")
pd.DataFrame(gs_cart.cv_results_).to_csv(
    ART/f"cv_results_tree_cart_{today}.csv", index=False
)
json.dump(
    [s if isinstance(s, str) else s.__class__.__name__
     for s in param_grid_cart["sampler"]],
    open(ART/"sampler_grid_cart.json","w"), indent=2
)

# Copy original ExtraTrees CSV
old_csv = max(Path().glob("cv_results_tree_*.csv"))
copy_fp = ART/f"{old_csv.stem}_extratree.csv"
if not copy_fp.exists():
    copy_fp.write_bytes(old_csv.read_bytes())

# Test-set Predictions saved twice
y_prob = gs_cart.best_estimator_.predict_proba(X_te)[:,1]
y_pred = (y_prob >= 0.50).astype(int)
np.save(ART/"y_prob_tree_cart.npy", y_prob)
np.save(ART/"y_pred_tree_cart.npy", y_pred)
np.save("y_prob_tree.npy", y_prob)
np.save("y_pred_tree.npy", y_pred)

if time.perf_counter() - _section_start > 360:
    raise TimeoutError("CART tuning section exceeded 6 minutes")

# 3. 15-Fold CV Summary on Training (≈60 s)
cv15 = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=SEED)
res = cross_validate(
    gs_cart.best_estimator_, X_tr, y_tr,
    cv=cv15,
    scoring={
      "roc_auc":"roc_auc",
      "pr_auc":"average_precision",
      "brier":"neg_brier_score",
      "bal_acc":"balanced_accuracy"
    },
    n_jobs=-1,
)

folds_cart = pd.DataFrame({
    "roc_auc": res["test_roc_auc"],
    "pr_auc":  res["test_pr_auc"],
    "brier":  -res["test_brier"],
    "bal_acc": res["test_bal_acc"],
})
folds_cart.to_csv(ART/"folds_cart.csv", index=False)

summary_metrics_cart = folds_cart.agg(["mean","std"]).round(3)
summary_metrics_cart.to_csv(ART/"summary_metrics_cart.csv")

plt.figure(figsize=(5,3))
plt.boxplot(
    [folds_cart.roc_auc, folds_cart.pr_auc],
    positions=[1,2],
    tick_labels=["ROC-AUC","PR-AUC"]
)
plt.ylabel("Score")
plt.tight_layout()
plt.savefig(ART/"roc_pr_box_cart.png")
plt.close()

if time.perf_counter() - _section_start > 360:
    raise TimeoutError("CART tuning section exceeded 6 minutes")

# 4. Statistical Comparison vs. Logistic Regression
if "folds" in globals():
    lr_auc = folds.query("model=='LR'")["roc_auc"].values
    dt_auc = folds_cart["roc_auc"].values
    p_auc = wilcoxon(lr_auc, dt_auc).pvalue
    cliff = np.sign(dt_auc - lr_auc).mean()
    summary_metrics_cart.loc["stat", ["roc_auc","pr_auc"]] = [p_auc, cliff]
    summary_metrics_cart.to_csv(ART/"summary_metrics_cart.csv")

# 5. Fairness Audit on Training
y_pred_train = gs_cart.best_estimator_.predict(X_tr)

group_col = next(
    (c for c in ["loan_term_bin","education","self_employed"] if c in X_tr.columns),
    None
)
if group_col:
    grp = X_tr[group_col].astype(str).fillna("NA")
    tprs = []
    for g in grp.unique():
        mask = grp == g
        tp = ((y_pred_train[mask] == 1) & (y_tr.values[mask] == 1)).sum()
        fn = ((y_pred_train[mask] == 0) & (y_tr.values[mask] == 1)).sum()
        tprs.append(tp / max(1, tp + fn))
    plt.figure(figsize=(2.5,3))
    plt.bar(grp.unique(), tprs)
    plt.axhline(0.8, ls="--")
    plt.ylabel("TPR")
    plt.tight_layout()
    plt.savefig(ART/"fairness_bar_cart.png")
    plt.close()

# 6. SHA-256 Manifest & Diagnostics
def _sha(fp): return sha256(fp.read_bytes()).hexdigest()[:12]

with open(ART/"SHA256_manifest.txt","a") as fh:
    for fp in ART.iterdir():
        if not fp.is_file():
            continue
        prefix = "extratree__" if fp.name.endswith("_extratree.csv") else "cart__"
        fh.write(f"{prefix}{_sha(fp)}  {fp.name}\n")

clf = gs_cart.best_estimator_.named_steps["clf"]
print(f"✔ Best ROC-AUC : {gs_cart.best_score_:.4f}")
print(f"✔ Tree depth   : {clf.get_depth()} | Leaves : {clf.get_n_leaves()}")

# 7. Final 6 min guard
total_elapsed = time.perf_counter() - _section_start
if total_elapsed > 360:
    raise TimeoutError(f"CART tuning section exceeded 6 min ({total_elapsed:.1f}s)")



# ### Best tuned model

import time
t0 = time.time()

# 1) Fit & capture best estimator
gs_cart.fit(X_tr, y_tr)
best_cart = gs_cart.best_estimator_

# 2) Display tuned hyperparameters
params = gs_cart.best_params_
print("Best CART hyperparameters:")
print(f"  • max_depth        = {params['clf__max_depth']}")
print(f"  • min_samples_leaf = {params['clf__min_samples_leaf']}")
print(f"  • ccp_alpha        = {params['clf__ccp_alpha']:.4f}")
print(f"  • class_weight     = {params['clf__class_weight']!r}")
print(f"  • sampler          = {params['sampler']!r}")

# 3) Save test-set predictions
import numpy as np
y_prob = best_cart.predict_proba(X_te)[:, 1]
y_pred = (y_prob >= 0.5).astype(int)
np.save("y_prob_tree.npy", y_prob)
np.save("y_pred_tree.npy", y_pred)

# 4) Hold-out evaluation + 95% bootstrap CIs
from sklearn.metrics import (
    roc_auc_score, average_precision_score,
    brier_score_loss, f1_score, recall_score,
    balanced_accuracy_score, roc_curve
)

# Load
y_prob = np.load("y_prob_tree.npy")
y_true = np.load("artefacts/y_test.npy")

# Youden-J threshold
fpr, tpr, thr = roc_curve(y_true, y_prob)
youden_thr = thr[np.argmax(tpr - fpr)]

def eval_at(th):
    y_pred = (y_prob >= th).astype(int)
    return {
        "ROC-AUC": roc_auc_score(y_true, y_prob),
        "PR-AUC": average_precision_score(y_true, y_prob),
        "Brier": brier_score_loss(y_true, y_prob),
        "F1": f1_score(y_true, y_pred),
        "Recall": recall_score(y_true, y_pred),
        "BalAcc": balanced_accuracy_score(y_true, y_pred),
    }

m50 = eval_at(0.50)
my  = eval_at(youden_thr)

# Bootstrap for CIs
rng = np.random.default_rng(0)
pos_idx = np.where(y_true == 1)[0]
neg_idx = np.where(y_true == 0)[0]
roc_bs = []
pr_bs  = []
for _ in range(1000):
    bidx = np.concatenate([
        rng.choice(pos_idx, size=len(pos_idx), replace=True),
        rng.choice(neg_idx, size=len(neg_idx), replace=True)
    ])
    roc_bs.append(roc_auc_score(y_true[bidx], y_prob[bidx]))
    pr_bs.append(average_precision_score(y_true[bidx], y_prob[bidx]))
roc_ci = np.percentile(roc_bs, [2.5, 97.5])
pr_ci  = np.percentile(pr_bs,  [2.5, 97.5])

# Print metrics
print(f"Test ROC-AUC = {m50['ROC-AUC']:.3f} (95% CI {roc_ci[0]:.3f}–{roc_ci[1]:.3f})")
print(f"Test PR-AUC  = {m50['PR-AUC']:.3f} (95% CI {pr_ci[0]:.3f}–{pr_ci[1]:.3f})")
print(f"Brier score = {m50['Brier']:.3f}")
print(f"F1@0.50      = {m50['F1']:.3f}   Recall@0.50 = {m50['Recall']:.3f}   BalAcc@0.50 = {m50['BalAcc']:.3f}")
print(f"F1@Youden    = {my['F1']:.3f}   Recall@Youden = {my['Recall']:.3f}   BalAcc@Youden = {my['BalAcc']:.3f}")

# 5) Top-10 feature importances
import pandas as pd
import matplotlib.pyplot as plt

names = prep_tree.get_feature_names_out()
imps  = best_cart.named_steps["clf"].feature_importances_
df_imp = pd.DataFrame({"feature": names, "importance": imps}).nlargest(10, "importance")
df_imp.to_csv("artefacts/feature_importance_cart.csv", index=False)

plt.figure(figsize=(6,4))
plt.barh(df_imp["feature"], df_imp["importance"])
plt.xlabel("Importance")
plt.title("CART Top-10 Features")
plt.tight_layout()
plt.savefig("artefacts/feature_importance_cart.png", dpi=300)
plt.close()

# 6) Test-set fairness audit (four-fifths rule)
group_col = "education" if "education" in X_te.columns else X_te.columns[0]
grp = X_te[group_col].astype(str).fillna("NA")

tprs = []
for g in grp.unique():
    mask = grp == g
    tp = ((y_prob[mask] >= youden_thr) & (y_true[mask] == 1)).sum()
    fn = ((y_prob[mask] <  youden_thr) & (y_true[mask] == 1)).sum()
    tprs.append(tp / max(1, tp + fn))
ratio = min(tprs) / max(tprs)

plt.figure(figsize=(3,3))
plt.bar(grp.unique(), tprs)
plt.axhline(0.8, ls="--", color="red")
plt.ylabel("TPR")
plt.title(f"4/5 TPR Ratio ({group_col}) = {ratio:.2f}")
plt.xticks(rotation=30)
plt.tight_layout()
plt.savefig("artefacts/fairness_bar_cart_test.png", dpi=300)
plt.close()

print(f"Four-fifths TPR ratio ({group_col}) = {ratio:.2f} {'✓' if ratio >= 0.8 else '⚠'}")

# 7) Append new artifacts to manifest
import hashlib, pathlib
man = pathlib.Path("artefacts/SHA256_manifest.txt")
for fn in [
    "feature_importance_cart.csv",
    "feature_importance_cart.png",
    "fairness_bar_cart_test.png"
]:
    path = pathlib.Path("artefacts") / fn
    h = hashlib.sha256(path.read_bytes()).hexdigest()[:12]
    man.write_text(man.read_text() + f"\ncart__{h}  artefacts/{fn}")

# 8) Final summary & runtime
elapsed = time.time() - t0
print("\n✔ Hyperparameters above.")
print(f"✔ Test ROC-AUC = {m50['ROC-AUC']:.3f} (95% CI {roc_ci[0]:.3f}–{roc_ci[1]:.3f})")
print("✔ Feature-importance & fairness charts saved.")
print(f"Section runtime: {elapsed:.0f} s (≤ 360 s OK)")


# ### Model Evaluation on Test Set
# === Model Evaluation on Test Set (CART) ===

# 0. Imports & Timer
import os
import json
import hashlib
from time import perf_counter
from pathlib import Path

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.metrics import (
    roc_auc_score, average_precision_score, brier_score_loss,
    f1_score, recall_score, balanced_accuracy_score,
    confusion_matrix, precision_recall_curve, roc_curve
)
from sklearn.calibration import calibration_curve

t0 = perf_counter()

# 1. Load predictions & true labels (≤ 1 s)
y_prob = np.load("y_prob_tree.npy")
y_pred = np.load("y_pred_tree.npy")
y_test = np.load("artefacts/y_test.npy")

assert y_prob.shape[0] == y_test.shape[0] == y_pred.shape[0], "Length mismatch"
assert not np.isnan(y_prob).any() and not np.isnan(y_test).any(), "NaNs detected"

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 2. Point‐estimate metrics @0.50 & Youden‐J (≤ 1 s)
fpr, tpr, thr = roc_curve(y_test, y_prob)
youden_idx = np.argmax(tpr - fpr)
t_youden   = thr[youden_idx]

def eval_metrics(y_true, y_prob, y_pred, suffix):
    cm = confusion_matrix(y_true, y_pred)
    tn, fp, fn, tp = cm.ravel()
    return {
        f"ROC_AUC{suffix}":     roc_auc_score(y_true, y_prob),
        f"PR_AUC{suffix}":      average_precision_score(y_true, y_prob),
        f"Brier{suffix}":       brier_score_loss(y_true, y_prob),
        f"F1{suffix}":          f1_score(y_true, y_pred),
        f"Recall{suffix}":      recall_score(y_true, y_pred),
        f"Specificity{suffix}": tn / (tn + fp),
        f"BalAcc{suffix}":      balanced_accuracy_score(y_true, y_pred)
    }

metrics_050    = eval_metrics(y_test, y_prob, y_pred, "@0.50")
y_pred_youden = (y_prob >= t_youden).astype(int)
metrics_Y      = eval_metrics(y_test, y_prob, y_pred_youden, "@Youden")

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 3. Stratified bootstrap → 95 % CIs & p‐values (≈ 60 s)
n_boot = 1000
if perf_counter() - t0 > 300:
    n_boot = 500

rng     = np.random.default_rng(0)
idx_pos = np.where(y_test == 1)[0]
idx_neg = np.where(y_test == 0)[0]

roc_b, pr_b = [], []
for _ in range(n_boot):
    boot_idx = np.concatenate([
        rng.choice(idx_pos, size=len(idx_pos), replace=True),
        rng.choice(idx_neg, size=len(idx_neg), replace=True),
    ])
    roc_b.append(roc_auc_score(y_test[boot_idx], y_prob[boot_idx]))
    pr_b.append(average_precision_score(y_test[boot_idx], y_prob[boot_idx]))

ROC_CI = np.percentile(roc_b, [2.5, 97.5]).tolist()
PR_CI  = np.percentile(pr_b,  [2.5, 97.5]).tolist()
ROC_p  = 2 * min((np.array(roc_b) < 0.5).mean(), (np.array(roc_b) > 0.5).mean())
PR_p   = 2 * min((np.array(pr_b) < y_test.mean()).mean(),
                 (np.array(pr_b) > y_test.mean()).mean())
bootstrap_iters = n_boot
print(f"Bootstrap iterations used: {bootstrap_iters}")

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 4. Calibration curve & Murphy Brier split (≤ 10 s)
prob_true, prob_pred = calibration_curve(y_test, y_prob, n_bins=10)

calib_path = Path("artefacts/calibration_cart.png")
if calib_path.exists():
    img = plt.imread(calib_path)
    plt.figure(figsize=(3,3))
    plt.imshow(img)
    plt.axis("off")
    plt.close()
else:
    plt.figure(figsize=(3,3))
    plt.plot(prob_pred, prob_true, "o-")
    plt.plot([0,1], [0,1], "--", lw=0.8)
    plt.xlabel("Predicted Probability")
    plt.ylabel("Observed Frequency")
    plt.tight_layout()
    plt.savefig(calib_path)
    plt.close()

uncertainty = y_test.mean() * (1 - y_test.mean())
reliability = ((prob_pred - prob_true)**2).mean()
resolution  = ((prob_true - y_test.mean())**2).mean()

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 5. Key visualizations (≤ 30 s)
def plot_or_load(fname, plot_fn):
    p = Path("artefacts") / fname
    if p.exists():
        img = plt.imread(p)
        plt.figure(); plt.imshow(img); plt.axis("off"); plt.close()
    else:
        plot_fn(); plt.savefig(p); plt.close()

# ROC
plot_or_load("roc_cart.png", lambda: (
    plt.figure(figsize=(3,3)),
    plt.plot(fpr, tpr),
    plt.plot([0,1],[0,1],"--",lw=0.8),
    plt.xlabel("FPR"), plt.ylabel("TPR")
))
# PR
precisions, recalls, _ = precision_recall_curve(y_test, y_prob)
plot_or_load("pr_cart.png", lambda: (
    plt.figure(figsize=(3,3)),
    plt.plot(recalls, precisions),
    plt.axhline(y_test.mean(), ls="--", lw=0.8),
    plt.xlabel("Recall"), plt.ylabel("Precision")
))
# Confusion matrix
plot_or_load("cm_cart.png", lambda: (
    plt.figure(figsize=(2.5,2)),
    sns.heatmap(confusion_matrix(y_test, y_pred_youden),
                annot=True, fmt="d", cbar=False),
    plt.ylabel("True Label"), plt.xlabel("Predicted Label")
))
# Feature importance & fairness (reuse)
plot_or_load("feature_importance_cart.png", lambda: None)
plot_or_load("fairness_bar_cart_test.png", lambda: None)

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 6. Fairness audit (≤ 5 s)
split_idx = np.load("artefacts/split_idx.npz")["test"]
raw = pd.read_csv("loan_approval_dataset.csv")
raw.columns = (
    raw.columns.str.strip()
               .str.lower()
               .str.replace(r"\s+", "_", regex=True)
)
X_te    = raw.iloc[split_idx]
groups  = X_te["education"].astype(str).fillna("NA")

tprs = []
for g in np.unique(groups):
    mask = groups == g
    tp   = ((y_prob[mask] >= t_youden) & (y_test[mask] == 1)).sum()
    fn   = ((y_prob[mask] <  t_youden) & (y_test[mask] == 1)).sum()
    tprs.append(tp/(tp+fn) if (tp+fn)>0 else 0)
four_fifths = min(tprs)/max(tprs) if len(tprs)>1 else 1.0

fair_path = Path("artefacts/fairness_bar_cart_test.png")
if not fair_path.exists():
    plt.figure(figsize=(3,3))
    plt.bar(np.unique(groups), tprs)
    plt.axhline(0.8, color="red", ls="--")
    plt.ylabel("TPR"); plt.xticks(rotation=30, ha="right")
    plt.tight_layout(); plt.savefig(fair_path); plt.close()

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 7. Persist JSON & update manifest (≤ 1 s)
out_path = Path("artefacts/test_metrics_cart.json")
if not out_path.exists():
    summary = {}
    summary.update(metrics_050)
    summary.update(metrics_Y)
    summary["ROC_CI"]      = ROC_CI
    summary["PR_CI"]       = PR_CI
    summary["ROC_p"]       = ROC_p
    summary["PR_p"]        = PR_p
    summary["Brier_split"] = {
        "reliability": reliability,
        "resolution": resolution,
        "uncertainty": uncertainty
    }
    summary["Fairness_ratio"]  = four_fifths
    summary["bootstrap_iters"] = bootstrap_iters
    out_path.write_text(json.dumps(summary, indent=2))
else:
    summary = json.load(open(out_path))

# 7b. SHA-256 manifest update (skip directories)
man_path = Path("artefacts/SHA256_manifest.txt")
if not man_path.exists():
    man_path.write_text("")

lines = set(man_path.read_text().splitlines())

for f in Path("artefacts").iterdir():
    if not f.is_file():
        continue
    tag   = f"cart__{hashlib.sha256(f.read_bytes()).hexdigest()[:12]}"
    entry = f"{tag}  artefacts/{f.name}"
    if entry not in lines:
        lines.add(entry)

man_path.write_text("\n".join(sorted(lines)))

elapsed = perf_counter() - t0
if elapsed > 350:
    raise RuntimeError(f"Time budget exceeded: {elapsed:.1f}s")

# 8. Console summary & runtime (≤ 1 s)
print("CART Test-Set Evaluation Summary:")
for k, v in summary.items():
    print(f"{k:25s}: {v}")
elapsed = perf_counter() - t0
print(f"\nCompleted in {elapsed:.1f}s (≤360s OK)")



# ### Feature importance
import time
import datetime
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

t0 = time.time()
ts = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

# 1. Extract feature names
names_lr   = prep_lr.get_feature_names_out()
names_cart = prep_tree.get_feature_names_out()

# 2. Compute importances
importances_cart = best_cart.named_steps["clf"].feature_importances_

coefs_lr       = best_lr.named_steps["clf"].coef_.ravel()
abs_beta_lr    = np.abs(coefs_lr)
odds_ratios_lr = np.exp(coefs_lr)

# 3. Build & save top-10 DataFrames with timestamped filenames
df_cart = (
    pd.DataFrame({
        "feature":    names_cart,
        "importance": importances_cart
    })
    .nlargest(10, "importance")
)
cart_csv = f"artefacts/feature_importance_cart_{ts}.csv"
df_cart.to_csv(cart_csv, index=False)

df_lr = (
    pd.DataFrame({
        "feature":  names_lr,
        "abs_beta": abs_beta_lr,
        "OR":       odds_ratios_lr
    })
    .nlargest(10, "abs_beta")
)
lr_csv = f"artefacts/oddsratio_table_lr_{ts}.csv"
df_lr.to_csv(lr_csv, index=False)

# 4. Plot & save bar charts with timestamped filenames
cart_png = f"artefacts/feature_importance_cart_{ts}.png"
plt.figure(figsize=(6, 4))
plt.barh(df_cart["feature"], df_cart["importance"])
plt.xlabel("Gini importance")
plt.title("CART: Top-10 Features")
plt.tight_layout()
plt.savefig(cart_png, dpi=300)
plt.close()

lr_png = f"artefacts/oddsratio_lr_{ts}.png"
plt.figure(figsize=(6, 4))
plt.barh(df_lr["feature"], df_lr["OR"])
plt.axvline(1, color="k", lw=0.8)
plt.xlabel("Odds ratio")
plt.title("LogReg: Top-10 Predictors")
plt.tight_layout()
plt.savefig(lr_png, dpi=300)
plt.close()

# 5. Runtime check
print(f"Feature importance section completed in {time.time() - t0:.2f} s")
print(f"Generated files:\n  {cart_csv}\n  {cart_png}\n  {lr_csv}\n  {lr_png}")


#REPORT
#!/usr/bin/env python3
import logging
import json
import shutil
from pathlib import Path

import pandas as pd
import numpy as np

def main():
    print("Starting report artifact collection...")

    # ── Configuration & Logging ────────────────────────────────────────────────
    logging.basicConfig(
        level=logging.INFO,
        format="%(asctime)s %(levelname)s: %(message)s",
        datefmt="%H:%M:%S",
    )

    ROOT  = Path(".")
    ART   = ROOT / "artefacts"
    PLOTS = ROOT / "plots"
    OUT   = ROOT / "report_artifacts"
    OUT.mkdir(parents=True, exist_ok=True)

    # ── Build file index for fast lookup ───────────────────────────────────────
    file_index: dict[str, list[Path]] = {}
    for d in (ROOT, ART, PLOTS):
        for p in d.rglob("*"):
            if p.is_file():
                file_index.setdefault(p.name, []).append(p)
    for paths in file_index.values():
        paths.sort(key=lambda p: p.stat().st_mtime, reverse=True)

    def find_path(name: str) -> Path | None:
        return file_index.get(name, [None])[0]

    def read_latest_glob(pattern: str, dirs=(ART, PLOTS, ROOT)) -> Path | None:
        candidates = []
        for d in dirs:
            candidates.extend(d.glob(pattern))
        return max(candidates, key=lambda p: p.stat().st_mtime) if candidates else None

    def write_section(f, title: str, reader_fn):
        underline = "—" * min(len(title), 80)
        logging.info(f"Writing section: {title}")
        f.write(f"• {title}\n{underline}\n")
        try:
            reader_fn(f)
        except FileNotFoundError as e:
            msg = f"**WARNING**: {e}"
            f.write(msg + "\n")
            logging.warning(msg)
        except Exception as e:
            msg = f"**ERROR**: {e}"
            f.write(msg + "\n")
            logging.error(msg, exc_info=True)
        f.write("\n")

    def flatten_cv(path: Path) -> pd.DataFrame:
        df = pd.read_csv(path, header=[0,1], index_col=0)
        df = df.dropna(how="all").dropna(how="all", axis=1)
        df.columns = [f"{lvl0}_{lvl1}".strip("_") for lvl0,lvl1 in df.columns]
        return df.round(4).loc[["DT","LR"]]

    def flatten_metrics(md: dict) -> dict:
        out = {}
        for k,v in md.items():
            if k == "bootstrap_iters":
                out[k] = int(v)
            elif isinstance(v, list) and len(v) == 2:
                out[f"{k}_low"]  = round(v[0], 4)
                out[f"{k}_high"] = round(v[1], 4)
            elif isinstance(v, dict):
                for subk, subv in v.items():
                    out[f"{k}_{subk}"] = round(subv, 4)
            elif isinstance(v, float):
                out[k] = int(v) if v.is_integer() else round(v, 4)
            else:
                out[k] = v
        return out

    # ── Reader functions ───────────────────────────────────────────────────────
    def dump_dataset_overview(ff):
        # get approved/rejected and total from split_info.json if available
        info = find_path("split_info.json")
        if info:
            data = json.load(info.open())
            total = data["n_total"]
            counts = {int(k): v for k, v in data["class_counts"].items()}
            approved, rejected = counts.get(1, 0), counts.get(0, 0)
            missing = False
        else:
            csv = find_path("loan_approval_dataset.csv")
            if not csv:
                raise FileNotFoundError("no split_info.json or raw CSV")
            df_full = pd.read_csv(csv)
            total = len(df_full)
            # map loan_status to 1/0
            mapping = {"approved": 1, "rejected": 0}
            status = df_full["loan_status"].astype(str).str.lower().map(mapping)
            approved = int((status == 1).sum())
            rejected = total - approved
            missing = df_full.isnull().any().any()

        # always count raw features by reading header only
        csv_path = find_path("loan_approval_dataset.csv")
        if not csv_path:
            raise FileNotFoundError("raw CSV not found for column count")
        n_raw_cols = len(pd.read_csv(csv_path, nrows=0).columns)

        p_app = approved / total * 100
        p_rej = rejected / total * 100
        miss_msg = "Some missing values present." if missing else "No missing values."

        ff.write(
            f"Dataset: Archit Sharma’s Kaggle Loan Approval Prediction\n"
            f"  • {total} rows  •  {n_raw_cols} raw features\n"
            f"  • Approved: {approved} ({p_app:.1f}%)  Rejected: {rejected} ({p_rej:.1f}%)\n"
            f"  • {miss_msg}"
        )

    def dump_lr_params(ff):
        p = read_latest_glob("cv_results_lr_*.csv")
        if not p:
            raise FileNotFoundError("no LR CV results")
        df = pd.read_csv(p)
        best = df[df["rank_test_roc_auc"] == 1].iloc[0]
        params = {
            "penalty": best["param_clf__penalty"],
            "C": best["param_clf__C"],
            "class_weight": best["param_clf__class_weight"],
            "sampler": best["param_sampler"],
        }
        ff.write(", ".join(f"{k}={v}" for k, v in params.items()))

    def dump_cart_params(ff):
        p = read_latest_glob("cv_results_tree_cart_*.csv")
        if not p:
            raise FileNotFoundError("no CART CV results")
        df = pd.read_csv(p)
        best = df[df["rank_test_roc_auc"] == 1].iloc[0]
        params = {
            "max_depth": best["param_clf__max_depth"],
            "min_samples_leaf": best["param_clf__min_samples_leaf"],
            "ccp_alpha": best["param_clf__ccp_alpha"],
            "class_weight": best["param_clf__class_weight"],
            "sampler": best["param_sampler"],
        }
        ff.write(", ".join(f"{k}={v}" for k, v in params.items()))

    def dump_feature_counts(ff):
        p = find_path("feature_registry.json")
        if not p:
            raise FileNotFoundError("feature_registry.json missing")
        reg = json.load(p.open())
        num = len(reg.get("numeric", []))
        cat = len(reg.get("categorical", []))
        dum = len(reg.get("dummy_0_1", []))
        ff.write(
            f"Total engineered features: {num + cat + dum} "
            f"(numeric={num}, categorical={cat}, dummy={dum})"
        )

    def dump_corr_top10(ff):
        p = find_path("numerical_correlations.csv")
        if not p:
            raise FileNotFoundError("numerical_correlations.csv missing")
        corr = pd.read_csv(p)
        top = corr.assign(abs_coef=corr["coef"].abs()).nlargest(10, "abs_coef")
        top = top.loc[:, ["var1", "var2", "coef", "q"]]
        top.columns = ["feature1", "feature2", "ρ", "q_value"]
        ff.write(top.to_string(index=False, float_format="%.3f"))

    def dump_dropped_twins(ff):
        p = find_path("feature_registry.json")
        if not p:
            raise FileNotFoundError("feature_registry.json missing")
        dropped = json.load(p.open()).get("numeric_to_drop_lm", [])
        ff.write("\n".join(dropped) if dropped else "None dropped")

    def dump_skew_profile(ff):
        p = find_path("skew_profile.csv")
        if not p:
            raise FileNotFoundError("skew_profile.csv missing")
        df = pd.read_csv(p)
        heavy = int(df["heavy_tail"].sum())
        total = len(df)
        ff.write(
            f"Heavy-tailed features: {heavy}/{total} ({heavy/total*100:.1f}%)\n"
            f"Median outlier rate (>3σ): {df['out_pct'].median():.1f}%"
        )

    def dump_cart_overfit(ff):
        sum_p = read_latest_glob("summary_metrics_cart.csv")
        if not sum_p:
            raise FileNotFoundError("summary_metrics_cart.csv missing")
        cv_roc = pd.read_csv(sum_p, index_col=0).loc["mean", "roc_auc"]
        t_p = find_path("test_metrics_cart.json")
        if not t_p:
            raise FileNotFoundError("test_metrics_cart.json missing")
        md = json.load(t_p.open())
        for key in ("ROC_AUC", "ROC_AUC@Youden", "ROC_AUC@0.50"):
            if key in md:
                test_roc = md[key]
                break
        else:
            raise KeyError("no ROC_AUC key in test_metrics_cart.json")
        ff.write(f"CART CV vs Test ROC AUC gap: {cv_roc - test_roc:.4f}")

    # ── Generate pipeline_tables.txt ───────────────────────────────────────
    with open(OUT / "pipeline_tables.txt", "w") as f:
        write_section(f, "0. Dataset & class overview", dump_dataset_overview)
        write_section(f, "0A. Best hyperparameters (LR)", dump_lr_params)
        write_section(f, "0B. Best hyperparameters (CART)", dump_cart_params)
        write_section(f, "0C. Engineered-feature counts", dump_feature_counts)
        write_section(f, "0D. Top-10 absolute Spearman correlations", dump_corr_top10)
        write_section(f, "0E. Features dropped for high |ρ|", dump_dropped_twins)
        write_section(f, "0F. Skewness profile", dump_skew_profile)
        write_section(f, "1. Categorical Diagnostics", lambda ff: ff.write(
            pd.read_csv(find_path("categorical_diagnostics.csv"))
              .assign(
                  pct_rejected=lambda d: (d["pct_rejected"] * 100).round(1),
                  pct_approved=lambda d: (d["pct_approved"] * 100).round(1),
              )
              .loc[:, [
                  "feature", "test", "n_rejected", "n_approved",
                  "pct_rejected", "pct_approved", "chi2", "p_value", "cramers_V"
              ]]
              .round({"chi2": 4, "cramers_V": 4})
              .to_string(index=False)
        ))
        write_section(f, "2. Numeric Features Dropped", lambda ff: ff.write(
            "\n".join(
                json.load(find_path("scaler_registry.json").open())
                  .get("numeric_to_drop_lm", [])
            ) or "None"
        ))
        write_section(f, "3. Train/Test Split Sizes", lambda ff: ff.write(
            f"Train size: {len(np.load(find_path('split_idx.npz'))['train'])}\n"
            f"Test size:  {len(np.load(find_path('split_idx.npz'))['test'])}"
        ))
        write_section(f, "4. LR CV Metrics", lambda ff: ff.write(
            flatten_cv(read_latest_glob("summary_metrics.csv")).to_string()
        ))
        write_section(f, "5. CART CV Metrics", lambda ff: ff.write(
            pd.read_csv(read_latest_glob("summary_metrics_cart.csv"), index_col=0)
              .loc[["mean","std"], ["roc_auc","pr_auc","brier","bal_acc"]]
              .round(4).to_string()
        ))
        write_section(f, "6. LR Odds Ratios", lambda ff: ff.write(
            pd.read_csv(read_latest_glob("oddsratio_table_lr_*.csv"))
              .loc[:, ["feature","abs_beta","OR"]]
              .assign(
                  abs_beta=lambda d: d["abs_beta"].round(4),
                  OR=lambda d: d["OR"].apply(lambda x: f"{x:.3e}")
              )
              .to_string(index=False)
        ))
        write_section(f, "7. CART Feature Importances", lambda ff: ff.write(
            pd.read_csv(read_latest_glob("feature_importance_cart_*.csv"))
              .loc[:, ["feature","importance"]]
              .round(4).to_string(index=False)
        ))
        write_section(f, "8.1 Test Performance (LR)", lambda ff: ff.write(
            pd.Series(
                flatten_metrics(
                    json.load(find_path("test_metrics_lr.json").open())
                ),
                name="LogReg"
            ).to_frame().to_string()
        ))
        write_section(f, "8.2 Test Performance (CART)", lambda ff: ff.write(
            pd.Series(
                flatten_metrics(
                    json.load(find_path("test_metrics_cart.json").open())
                ),
                name="CART"
            ).to_frame().to_string()
        ))
        write_section(f, "9. Overfitting check (CART ROC AUC gap)", dump_cart_overfit)

    logging.info(f"✅ Tables written to {OUT/'pipeline_tables.txt'}")

    # ── Copy curated diagrams ─────────────────────────────────────────────────
    DIAGRAMS = [
        "roc_lr.png", "roc_cart.png",
        "pr_lr.png", "pr_cart.png",
        "calibration_lr.png", "calibration_cart.png",
        "cm_lr.png", "cm_cart.png",
    ]
    for name in DIAGRAMS:
        src = find_path(name)
        if not src:
            logging.warning(f"Diagram not found: {name}")
            continue
        dst = OUT / name
        if src.resolve() != dst.resolve():
            shutil.copy(src, dst)
            logging.info(f"Copied diagram: {name}")
        else:
            logging.info(f"Already up to date: {name}")

    logging.info(f"✅ Curated diagrams copied to {OUT}/")
    print("Report artifact collection complete.")

    # ── Zip & download in Colab ───────────────────────────────────────────────
    try:
#         from google.colab import files
        zip_path = ROOT / f"{OUT.name}.zip"
        shutil.make_archive(str(zip_path.with_suffix('')), 'zip', str(OUT))
        print(f"Zipping artifacts to {zip_path.name} …")
#         files.download(zip_path.name)
    except ImportError:
        print("⚠ Not running in Colab; download the 'report_artifacts' folder manually.")

if __name__ == "__main__":
    main()
